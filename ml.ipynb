{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "OUT_DIR = \"catboost_kfold_optuna\"\n",
    "N_SPLITS = 5\n",
    "N_TRIALS = 30\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "target_col = \"target\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "cat_cols = [col for col in X.columns if X[col].dtype == \"object\" or pd.api.types.is_categorical_dtype(X[col])]\n",
    "\n",
    "base_params = {\n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"task_type\": \"CPU\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eval_metric\": \"RMSE\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"l2_leaf_reg\": 5,\n",
    "    \"random_strength\": 1,\n",
    "    \"random_state\": SEED,\n",
    "    \"verbose\": False,\n",
    "    \"iterations\": 10000\n",
    "}\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = base_params.copy()\n",
    "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 4, 10)\n",
    "        params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        params[\"l2_leaf_reg\"] = trial.suggest_int(\"l2_leaf_reg\", 1, 20)\n",
    "        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0.0, 1.0)\n",
    "        params[\"random_strength\"] = trial.suggest_float(\"random_strength\", 0.0, 20.0)\n",
    "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "        params[\"min_data_in_leaf\"] = trial.suggest_int(\"min_data_in_leaf\", 1, 100)\n",
    "        params[\"border_count\"] = trial.suggest_int(\"border_count\", 32, 255)\n",
    "        model = CatBoostRegressor(**params)\n",
    "        train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n",
    "        val_pool = Pool(X_val, y_val, cat_features=cat_cols)\n",
    "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=100, use_best_model=True, verbose=False)\n",
    "        preds = model.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    best_params = study.best_params\n",
    "    final_params = base_params.copy()\n",
    "    final_params.update(best_params)\n",
    "    final_model = CatBoostRegressor(**final_params)\n",
    "    train_pool_full = Pool(X_train, y_train, cat_features=cat_cols)\n",
    "    val_pool_full = Pool(X_val, y_val, cat_features=cat_cols)\n",
    "    final_model.fit(train_pool_full, eval_set=val_pool_full, early_stopping_rounds=100, use_best_model=True, verbose=False)\n",
    "    val_preds = final_model.predict(X_val)\n",
    "    val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, \"cv_fold_results.csv\"), index=False)\n",
    "avg_rmse = results_df[\"rmse\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "OUT_DIR = \"lgb_kfold_optuna\"\n",
    "N_SPLITS = 5\n",
    "N_TRIALS = 40\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "target_col = \"target\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\" or pd.api.types.is_categorical_dtype(X[c])]\n",
    "cat_feat_indices = [X.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "base_params = {\n",
    "    \"objective\": \"mse\",\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"random_state\": SEED,\n",
    "    \"lambda_l2\": 5.0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 32,\n",
    "    \"extra_trees\": True,\n",
    "    \"metric\": \"mse\",\n",
    "}\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = base_params.copy()\n",
    "        params[\"num_leaves\"] = trial.suggest_int(\"num_leaves\", 8, 512)\n",
    "        params[\"lambda_l1\"] = trial.suggest_float(\"lambda_l1\", 0.0, 10.0)\n",
    "        params[\"lambda_l2\"] = trial.suggest_float(\"lambda_l2\", 0.0, 20.0)\n",
    "        params[\"min_data_in_leaf\"] = trial.suggest_int(\"min_data_in_leaf\", 1, 500)\n",
    "        params[\"feature_fraction\"] = trial.suggest_float(\"feature_fraction\", 0.4, 1.0)\n",
    "        params[\"bagging_fraction\"] = trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        params[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 0, 10)\n",
    "        params[\"extra_trees\"] = trial.suggest_categorical(\"extra_trees\", [True, False])\n",
    "        use_dart = trial.suggest_categorical(\"use_dart\", [False, True])\n",
    "        params[\"boosting_type\"] = \"dart\" if use_dart else \"gbdt\"\n",
    "        params[\"reg_sqrt\"] = trial.suggest_categorical(\"reg_sqrt\", [False, True])\n",
    "        params[\"min_split_gain\"] = trial.suggest_float(\"min_split_gain\", 0.0, 5.0)\n",
    "        params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        model = lgb.LGBMRegressor(**params, n_estimators=10000)\n",
    "        fit_params = {\"eval_set\": [(X_val, y_val)], \"early_stopping_rounds\": 100, \"verbose\": False}\n",
    "        if len(cat_cols) > 0:\n",
    "            fit_params[\"categorical_feature\"] = cat_cols\n",
    "        model.fit(X_train, y_train, **fit_params)\n",
    "        preds = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "        rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "    best_params = study.best_params\n",
    "    final_params = base_params.copy()\n",
    "    final_params.update(best_params)\n",
    "    final_model = lgb.LGBMRegressor(**final_params, n_estimators=10000)\n",
    "    fit_params = {\"eval_set\": [(X_val, y_val)], \"early_stopping_rounds\": 100, \"verbose\": False}\n",
    "    if len(cat_cols) > 0:\n",
    "        fit_params[\"categorical_feature\"] = cat_cols\n",
    "    final_model.fit(X_train, y_train, **fit_params)\n",
    "    val_preds = final_model.predict(X_val, num_iteration=final_model.best_iteration_)\n",
    "    val_rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
    "\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "results_df.to_csv(os.path.join(OUT_DIR, \"lgb_cv_fold_results.csv\"), index=False)\n",
    "avg_rmse = results_df[\"rmse\"].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "TEST_CSV = \"test.csv\"\n",
    "TIME_LIMIT = 600\n",
    "PRED_OUTPUT = \"test_predictions_autogluon.csv\"\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "target_col = \"target\"\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "df[target_col] = y\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[target_col], random_state=42)\n",
    "\n",
    "predictor = TabularPredictor(label=target_col, problem_type=\"classification\").fit(\n",
    "    train_data=train_df,\n",
    "    tuning_data=val_df,\n",
    "    time_limit=TIME_LIMIT,\n",
    "    presets=\"medium_quality_faster_train\"\n",
    ")\n",
    "\n",
    "eval_results = predictor.evaluate(val_df)\n",
    "\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "preds = predictor.predict(test_df)\n",
    "probs = predictor.predict_proba(test_df)\n",
    "out = test_df.copy()\n",
    "out[\"prediction\"] = preds\n",
    "if isinstance(probs, pd.DataFrame):\n",
    "    for col in probs.columns:\n",
    "        out[f\"prob_{col}\"] = probs[col].values\n",
    "else:\n",
    "    out[\"probability\"] = probs\n",
    "out.to_csv(PRED_OUTPUT, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# с указанием типов данных\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "feature_types = {'user_id':'category', 'age':'int', 'comment':'text'}\n",
    "predictor = TabularPredictor(label='target').fit(df, feature_types=feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Быстрые параметры fit() — что важно менять\n",
    "\n",
    "time_limit — общий таймаут (секунды) на весь fit(); удобно для итераций/прототипов.\n",
    "\n",
    "presets — наборы конфигураций (например, 'medium_quality_faster_train', 'best_quality', 'high_quality_fast_inference', extreme и т.д.). Presets задают: какие модели, bagging, stack levels, HPO defaults и пр. Начни с пресета для быстрой разработки, затем переключись на best_quality для финального прогона. \n",
    "\n",
    "\n",
    "hyperparameters — словарь, где ключи = модели (GBM, CAT, XGBoost, RF, NN_TORCH и т.д.), значения — либо список конфигураций, либо dict гиперпараметров (включая поисковые пространства через autogluon.core объекты). Пример см. ниже. \n",
    "\n",
    "\n",
    "hyperparameter_tune_kwargs — включить HPO (число trial'ов, стратегия поиска/шедьюлер). Пример: {'searcher':'random','scheduler':'local','num_trials':50} (точные ключи зависят от версии AutoGluon; searcher часто обязателен). \n",
    "\n",
    "\n",
    "ag_args_fit — внутренняя настройка обучения конкретной модели (num_cpus, num_gpus, max_rows, max_features и т.п.). Полезно для контроля ресурсов.\n",
    "\n",
    "Масштабирование: если у тебя много строк/фич — контролируй max_rows/max_features и используешь num_cpus/num_gpus.\n",
    "\n",
    "Когда ручной контроль нужен: при специфичных фичах/лейблах/пропусках — лучше подготовить фичи заранее и передать feature_types, либо отключать встроенные трансформации для некоторых моделей через hyperparameters/ag_args_fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'GBM': {'num_boost_round': 1000, 'learning_rate': 0.03},\n",
    "    'NN_TORCH': {'num_epochs': 10},\n",
    "    'CAT': {'iterations': 1000},\n",
    "    'KNN': {}, 'RF': {}\n",
    "}\n",
    "hpo = {'searcher':'random', 'scheduler':'local', 'num_trials': 20}\n",
    "predictor = TabularPredictor(label='target').fit(\n",
    "    train_data=train_df,\n",
    "    time_limit=3600,\n",
    "    presets='best_quality',\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hpo,\n",
    "    ag_args_fit={'num_cpus':8, 'num_gpus':1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_bag_folds — число фолдов для bagging; по умолчанию >0 для многих пресетов. Увеличение повышает стабильность/качество, но пропорционально увеличивает время обучения и inference complexity.\n",
    "\n",
    "num_bag_sets — сколько раз повторить bagging (repeats).\n",
    "\n",
    "Если тебе важна inference-скорость, после тренировки можно вызвать predictor.collapse_ensemble() / refit_full() (или удалить модели, оставить самый сильный) — для резкого уменьшения времени inference с небольшим падением качества.\n",
    "\n",
    "AutoGluon поддерживает sample_weight: можно передать имя столбца с весами при создании TabularPredictor (sample_weight='w') или в fit/evaluate. Также есть удобные значения 'auto_weight' и 'balance_weight'. Это полезно при сильном дисбалансе.\n",
    "\n",
    "eval_metric можно задать при создании TabularPredictor(label=..., eval_metric='f1'). Подбирай метрику, релевантную задаче (AUC/F1/accuracy/mae и т.д.).\n",
    "\n",
    "AutoGluon строит внутренние валидации/фолды; если хочешь control — передавай tuning_data (валидационная выборка) или указывай num_bag_folds/num_stack_levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
