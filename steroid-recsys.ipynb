{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439166ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==================== ДАТАСЕТЫ ====================\n",
    "class UniversalRecDataset(Dataset):\n",
    "    \"\"\"Универсальный датасет для всех типов задач рекомендаций\"\"\"\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 user_col: str = 'user_id',\n",
    "                 item_col: str = 'item_id', \n",
    "                 rating_col: Optional[str] = 'rating',\n",
    "                 time_col: Optional[str] = 'timestamp',\n",
    "                 sequence_length: int = 50,\n",
    "                 task_type: str = 'sequential',\n",
    "                 additional_features: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        additional_features: список дополнительных колонок для фич\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.rating_col = rating_col\n",
    "        self.time_col = time_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.task_type = task_type\n",
    "        self.additional_features = additional_features or []\n",
    "        \n",
    "        if task_type == 'sequential':\n",
    "            # Подготовка для последовательных задач\n",
    "            self.df = self.df.sort_values([user_col, time_col]).reset_index(drop=True)\n",
    "            self.user_sequences = defaultdict(list)\n",
    "            for _, row in self.df.iterrows():\n",
    "                user_id = row[user_col]\n",
    "                item_id = row[item_col]\n",
    "                seq_item = {'item_id': item_id}\n",
    "                for feat in self.additional_features:\n",
    "                    if feat in row:\n",
    "                        seq_item[feat] = row[feat]\n",
    "                self.user_sequences[user_id].append(seq_item)\n",
    "            self.users = list(self.user_sequences.keys())\n",
    "        else:\n",
    "            # Для других задач просто сохраняем все взаимодействия\n",
    "            pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.task_type == 'sequential':\n",
    "            return len(self.users)\n",
    "        else:\n",
    "            return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.task_type == 'sequential':\n",
    "            user_id = self.users[idx]\n",
    "            items = self.user_sequences[user_id]\n",
    "            \n",
    "            # Берем последние N взаимодействий\n",
    "            if len(items) > self.sequence_length:\n",
    "                sequence = items[-self.sequence_length:]\n",
    "            else:\n",
    "                # Паддинг первым элементом\n",
    "                first_item = items[0]\n",
    "                sequence = [first_item] * (self.sequence_length - len(items)) + items\n",
    "            \n",
    "            # Извлекаем айтемы и фичи\n",
    "            item_sequence = [item['item_id'] for item in sequence[:-1]]\n",
    "            target_item = [sequence[-1]['item_id']]\n",
    "            \n",
    "            batch = {\n",
    "                'user_id': int(user_id),\n",
    "                'sequence': torch.LongTensor(item_sequence),\n",
    "                'target_item': torch.LongTensor(target_item)\n",
    "            }\n",
    "            \n",
    "            # Добавляем дополнительные фичи если есть\n",
    "            if self.additional_features:\n",
    "                for feat in self.additional_features:\n",
    "                    feat_sequence = [item.get(feat, 0) for item in sequence[:-1]]\n",
    "                    batch[f'sequence_{feat}'] = torch.FloatTensor(feat_sequence)\n",
    "                    batch[f'target_{feat}'] = torch.FloatTensor([sequence[-1].get(feat, 0)])\n",
    "            \n",
    "            return batch\n",
    "        else:\n",
    "            row = self.df.iloc[idx]\n",
    "            result = {\n",
    "                'user_id': int(row[self.user_col]),\n",
    "                'item_id': int(row[self.item_col])\n",
    "            }\n",
    "            \n",
    "            if self.rating_col and self.rating_col in row:\n",
    "                result['rating'] = float(row[self.rating_col])\n",
    "            \n",
    "            if self.time_col and self.time_col in row:\n",
    "                result['timestamp'] = float(row[self.time_col])\n",
    "                \n",
    "            # Добавляем дополнительные фичи\n",
    "            for feat in self.additional_features:\n",
    "                if feat in row:\n",
    "                    result[feat] = float(row[feat])\n",
    "                    \n",
    "            return result\n",
    "\n",
    "# ==================== МОДЕЛИ ====================\n",
    "\n",
    "class SASEmbedding(nn.Module):\n",
    "    \"\"\"Эмбеддинги с позиционным кодированием для SASRec\"\"\"\n",
    "    def __init__(self, n_items: int, embedding_dim: int, max_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.item_embeddings = nn.Embedding(n_items, embedding_dim)\n",
    "        self.position_embeddings = nn.Embedding(max_len, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, item_ids: torch.Tensor, positions: Optional[torch.Tensor] = None):\n",
    "        if positions is None:\n",
    "            positions = torch.arange(item_ids.size(1), device=item_ids.device).unsqueeze(0)\n",
    "            positions = positions.expand_as(item_ids)\n",
    "            \n",
    "        item_emb = self.item_embeddings(item_ids)\n",
    "        pos_emb = self.position_embeddings(positions)\n",
    "        \n",
    "        embeddings = item_emb + pos_emb\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    \"\"\"Мощная sequential модель - SASRec (Self-Attentive Sequential Recommendation)\"\"\"\n",
    "    def __init__(self, n_items: int, embedding_dim: int = 128, n_heads: int = 8, \n",
    "                 n_layers: int = 2, dropout: float = 0.1, max_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = SASEmbedding(n_items, embedding_dim, max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.output_projection = nn.Linear(embedding_dim, n_items)\n",
    "        \n",
    "    def forward(self, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        # sequences: (batch_size, seq_len)\n",
    "        embedded = self.embeddings(sequences)  # (batch_size, seq_len, embed_dim)\n",
    "        attended = self.transformer(embedded)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Используем последнее состояние для предсказания\n",
    "        last_states = attended[:, -1, :]  # (batch_size, embed_dim)\n",
    "        logits = self.output_projection(last_states)  # (batch_size, n_items)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    \"\"\"DeepFM - мощная модель для факторизации и нейронных сетей\"\"\"\n",
    "    def __init__(self, n_users: int, n_items: int, embedding_dim: int = 64, \n",
    "                 deep_dims: List[int] = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # FM part - second order interactions\n",
    "        self.fm_linear = nn.Linear(n_users + n_items, 1)\n",
    "        self.fm_bias = nn.Parameter(torch.tensor(0.0))\n",
    "        \n",
    "        # Deep part\n",
    "        total_features = embedding_dim * 2  # concatenated user + item embeddings\n",
    "        layers = []\n",
    "        input_dim = total_features\n",
    "        for hidden_dim in deep_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # FM embeddings\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        \n",
    "        # Linear part\n",
    "        user_onehot = F.one_hot(user_ids, num_classes=self.user_embedding.num_embeddings).float()\n",
    "        item_onehot = F.one_hot(item_ids, num_classes=self.item_embedding.num_embeddings).float()\n",
    "        linear_input = torch.cat([user_onehot, item_onehot], dim=1)\n",
    "        linear_output = self.fm_linear(linear_input) + self.fm_bias\n",
    "        \n",
    "        # FM second order (simplified)\n",
    "        concat_emb = torch.cat([user_emb, item_emb], dim=1)\n",
    "        fm_output = torch.sum(user_emb * item_emb, dim=1, keepdim=True)\n",
    "        \n",
    "        # Deep part\n",
    "        deep_output = self.mlp(concat_emb)\n",
    "        \n",
    "        return linear_output + fm_output + deep_output\n",
    "\n",
    "class WideDeep(nn.Module):\n",
    "    \"\"\"Wide & Deep модель\"\"\"\n",
    "    def __init__(self, n_users: int, n_items: int, embedding_dim: int = 64,\n",
    "                 deep_dims: List[int] = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # Wide part - linear model\n",
    "        self.wide_linear = nn.Linear(n_users + n_items, 1)\n",
    "        \n",
    "        # Deep part - neural network\n",
    "        concat_dim = embedding_dim * 2\n",
    "        layers = []\n",
    "        input_dim = concat_dim\n",
    "        for hidden_dim in deep_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.deep_mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # Wide part\n",
    "        user_onehot = F.one_hot(user_ids, num_classes=self.user_embedding.num_embeddings).float()\n",
    "        item_onehot = F.one_hot(item_ids, num_classes=self.item_embedding.num_embeddings).float()\n",
    "        wide_input = torch.cat([user_onehot, item_onehot], dim=1)\n",
    "        wide_output = self.wide_linear(wide_input)\n",
    "        \n",
    "        # Deep part\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        item_emb = self.item_embedding(item_ids)\n",
    "        concat_emb = torch.cat([user_emb, item_emb], dim=1)\n",
    "        deep_output = self.deep_mlp(concat_emb)\n",
    "        \n",
    "        return wide_output + deep_output\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"LightGCN - графовая модель для рекомендаций\"\"\"\n",
    "    def __init__(self, n_users: int, n_items: int, embedding_dim: int = 64, n_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Инициализация эмбеддингов\n",
    "        self.user_embeddings = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # Инициализация\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        \n",
    "    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor, \n",
    "                adj_matrix: torch.sparse.FloatTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        adj_matrix: sparse adjacency matrix user-item interactions\n",
    "        \"\"\"\n",
    "        # Get initial embeddings\n",
    "        user_emb = self.user_embeddings.weight  # (n_users, embed_dim)\n",
    "        item_emb = self.item_embeddings.weight  # (n_items, embed_dim)\n",
    "        \n",
    "        # Concatenate user and item embeddings\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)  # (n_users + n_items, embed_dim)\n",
    "        \n",
    "        # Graph convolution\n",
    "        embeddings = [all_emb]\n",
    "        for _ in range(self.n_layers):\n",
    "            all_emb = torch.sparse.mm(adj_matrix, all_emb)\n",
    "            embeddings.append(all_emb)\n",
    "        \n",
    "        # Take mean of all layer embeddings (LightGCN)\n",
    "        final_emb = torch.stack(embeddings, dim=0).mean(dim=0)\n",
    "        \n",
    "        # Split back\n",
    "        user_final = final_emb[:self.n_users]\n",
    "        item_final = final_emb[self.n_users:]\n",
    "        \n",
    "        # Get embeddings for specific users and items\n",
    "        user_rep = user_final[user_ids]\n",
    "        item_rep = item_final[item_ids]\n",
    "        \n",
    "        # Dot product\n",
    "        return torch.sum(user_rep * item_rep, dim=1)\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \"\"\"Custom Multi-Head Attention для рекомендаций\"\"\"\n",
    "    def __init__(self, embedding_dim: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % n_heads == 0\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embedding_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fc_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fc_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.head_dim ** 0.5\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        x = torch.matmul(attention, V)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.embedding_dim)\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention\n",
    "\n",
    "class AdvancedSequentialModel(nn.Module):\n",
    "    \"\"\"Продвинутая sequential модель с attention и temporal features\"\"\"\n",
    "    def __init__(self, n_items: int, embedding_dim: int = 128, n_heads: int = 8,\n",
    "                 n_layers: int = 2, dropout: float = 0.1, max_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = SASEmbedding(n_items, embedding_dim, max_len)\n",
    "        \n",
    "        # Multi-head attention layers\n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            MultiHeadAttentionLayer(embedding_dim, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Feed forward layers\n",
    "        self.feed_forwards = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(embedding_dim * 4, embedding_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(embedding_dim) for _ in range(n_layers * 2)\n",
    "        ])\n",
    "        \n",
    "        self.output_projection = nn.Linear(embedding_dim, n_items)\n",
    "        \n",
    "    def forward(self, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embeddings(sequences)\n",
    "        \n",
    "        x = embedded\n",
    "        for i, (attn_layer, ff_layer, norm1, norm2) in enumerate(\n",
    "            zip(self.attention_layers, self.feed_forwards, \n",
    "                self.layer_norms[::2], self.layer_norms[1::2])\n",
    "        ):\n",
    "            # Self-attention\n",
    "            attn_out, _ = attn_layer(x, x, x)\n",
    "            x = norm1(x + attn_out)\n",
    "            \n",
    "            # Feed forward\n",
    "            ff_out = ff_layer(x)\n",
    "            x = norm2(x + ff_out)\n",
    "        \n",
    "        # Используем последнее состояние\n",
    "        last_states = x[:, -1, :]\n",
    "        logits = self.output_projection(last_states)\n",
    "        return F.log_softmax(logits, dim=1)\n",
    "\n",
    "# ==================== (COMBINED) ====================\n",
    "\n",
    "class UniversalPowerfulModel(nn.Module):\n",
    "    \"\"\"Универсальная мощная модель, комбинирующая разные подходы\"\"\"\n",
    "    def __init__(self, \n",
    "                 n_users: int, \n",
    "                 n_items: int,\n",
    "                 model_types: List[str],  # ['mf', 'deepfm', 'sasrec', 'lightgcn']\n",
    "                 embedding_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 n_heads: int = 8,\n",
    "                 n_layers: int = 2,\n",
    "                 sequence_length: int = 50):\n",
    "        super().__init__()\n",
    "        self.model_types = model_types\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Shared embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        self.models = nn.ModuleDict()\n",
    "        \n",
    "        for model_type in model_types:\n",
    "            if model_type == 'mf':\n",
    "                self.models['mf'] = nn.Sequential(\n",
    "                    nn.Linear(embedding_dim * 2, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(hidden_dim, 1)\n",
    "                )\n",
    "            elif model_type == 'deepfm':\n",
    "                self.models['deepfm'] = DeepFM(n_users, n_items, embedding_dim)\n",
    "            elif model_type == 'sasrec':\n",
    "                self.models['sasrec'] = SASRec(n_items, embedding_dim, n_heads, n_layers)\n",
    "            elif model_type == 'wide_deep':\n",
    "                self.models['wide_deep'] = WideDeep(n_users, n_items, embedding_dim)\n",
    "            elif model_type == 'advanced_seq':\n",
    "                self.models['advanced_seq'] = AdvancedSequentialModel(\n",
    "                    n_items, embedding_dim, n_heads, n_layers\n",
    "                )\n",
    "        \n",
    "        # Final combination layer\n",
    "        n_model_outputs = len(model_types)\n",
    "        self.combination_layer = nn.Linear(n_model_outputs, 1)\n",
    "        \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        outputs = []\n",
    "        \n",
    "        for model_type, model in self.models.items():\n",
    "            if model_type in ['sasrec', 'advanced_seq']:\n",
    "                # Sequential models\n",
    "                if 'sequence' in batch:\n",
    "                    output = model(batch['sequence'])\n",
    "                    # For sequential models, we return log probabilities\n",
    "                    # Extract score for target item\n",
    "                    target_item = batch['target_item'].squeeze(1)\n",
    "                    batch_size = output.size(0)\n",
    "                    scores = output[torch.arange(batch_size), target_item]\n",
    "                    outputs.append(scores.unsqueeze(1))\n",
    "            else:\n",
    "                # Other models\n",
    "                if 'user_id' in batch and 'item_id' in batch:\n",
    "                    if model_type in ['deepfm', 'wide_deep']:\n",
    "                        output = model(batch['user_id'], batch['item_id']).squeeze(1)\n",
    "                    else:\n",
    "                        user_emb = self.user_embedding(batch['user_id'])\n",
    "                        item_emb = self.item_embedding(batch['item_id'])\n",
    "                        concat_emb = torch.cat([user_emb, item_emb], dim=1)\n",
    "                        output = model(concat_emb).squeeze(1)\n",
    "                    outputs.append(output.unsqueeze(1))\n",
    "        \n",
    "        if outputs:\n",
    "            combined_output = torch.cat(outputs, dim=1)\n",
    "            final_output = self.combination_layer(combined_output).squeeze(1)\n",
    "            return final_output\n",
    "        else:\n",
    "            # Fallback\n",
    "            user_emb = self.user_embedding(batch['user_id'])\n",
    "            item_emb = self.item_embedding(batch['item_id'])\n",
    "            return torch.sum(user_emb * item_emb, dim=1)\n",
    "\n",
    "# ==================== МЕТРИКИ ====================\n",
    "\n",
    "class RecMetrics:\n",
    "    \"\"\"Класс для вычисления рекомендательных метрик\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Precision@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_set = set(y_pred)\n",
    "        return len(y_true_set & y_pred_set) / len(y_pred_set) if y_pred_set else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Recall@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_set = set(y_pred)\n",
    "        return len(y_true_set & y_pred_set) / len(y_true_set) if y_true_set else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def map_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Mean Average Precision@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        \n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "        \n",
    "        for i, p in enumerate(y_pred):\n",
    "            if p in y_true and p not in y_pred[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "        \n",
    "        if not y_true:\n",
    "            return 0.0\n",
    "        \n",
    "        return score / min(len(y_true), k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        \n",
    "        # Бинарная релевантность\n",
    "        relevance = [1.0 if item in y_true else 0.0 for item in y_pred]\n",
    "        \n",
    "        # DCG\n",
    "        dcg = sum(rel / np.log2(pos + 2) for pos, rel in enumerate(relevance))\n",
    "        \n",
    "        # IDCG\n",
    "        idcg = sum(1.0 / np.log2(pos + 2) for pos in range(min(len(y_true), k)))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0.0 else 0.0\n",
    "\n",
    "# ==================== LIGHTNING ====================\n",
    "\n",
    "class UniversalRecLightning(pl.LightningModule):\n",
    "    \"\"\"Универсальный Lightning модуль для всех задач рекомендаций\"\"\"\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 task_type: str = 'sequential',\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 metrics: List[str] = ['map']):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.metrics = metrics\n",
    "        self.metrics_calculator = RecMetrics()\n",
    "        \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        return self.model(batch)\n",
    "    \n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        if self.task_type == 'classic':\n",
    "            predictions = self.model(batch)\n",
    "            ratings = batch['rating']\n",
    "            loss = F.mse_loss(predictions, ratings)\n",
    "            \n",
    "        elif self.task_type == 'sequential':\n",
    "            # Для sequential моделей\n",
    "            if 'target_item' in batch:\n",
    "                logits = self.model(batch)\n",
    "                target_items = batch['target_item'].squeeze(1)\n",
    "                # Используем NLL loss для log_softmax\n",
    "                loss = F.nll_loss(logits, target_items)\n",
    "            else:\n",
    "                # Fallback для других sequential моделей\n",
    "                predictions = self.model(batch)\n",
    "                ratings = batch.get('rating', torch.ones(batch['user_id'].size(0)))\n",
    "                loss = F.mse_loss(predictions, ratings)\n",
    "                \n",
    "        elif self.task_type == 'implicit':\n",
    "            scores = self.model(batch)\n",
    "            loss = -F.logsigmoid(scores).mean()\n",
    "            \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(batch)\n",
    "            \n",
    "            if self.task_type == 'classic':\n",
    "                ratings = batch['rating']\n",
    "                val_loss = F.mse_loss(predictions, ratings)\n",
    "                self.log('val_loss', val_loss)\n",
    "                \n",
    "            elif self.task_type == 'sequential':\n",
    "                if 'target_item' in batch:\n",
    "                    target_items = batch['target_item'].squeeze(1)\n",
    "                    # Calculate accuracy for sequential models\n",
    "                    if hasattr(predictions, 'shape') and len(predictions.shape) > 1:\n",
    "                        accuracy = (predictions.argmax(dim=1) == target_items).float().mean()\n",
    "                        self.log('val_acc', accuracy)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Используем AdamW с весами для разных частей модели\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_epochs,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler\n",
    "        }\n",
    "\n",
    "# ==================== ТРЕЙНЕР ====================\n",
    "\n",
    "class UniversalRecTrainer:\n",
    "    \"\"\"Универсальный трейнер для всех задач рекомендаций\"\"\"\n",
    "    def __init__(self,\n",
    "                 model_type: str = 'mf',  # 'mf', 'deepfm', 'sasrec', 'lightgcn', 'wide_deep', 'advanced_seq', 'universal'\n",
    "                 task_type: str = 'sequential',\n",
    "                 embedding_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 sequence_length: int = 50,\n",
    "                 n_heads: int = 8,\n",
    "                 n_layers: int = 2,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 batch_size: int = 256,\n",
    "                 max_epochs: int = 50,\n",
    "                 model_types: Optional[List[str]] = None):  # для универсальной модели\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.task_type = task_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.model_types = model_types or [model_type]\n",
    "        \n",
    "        self.user_encoder = None\n",
    "        self.item_encoder = None\n",
    "        self.model = None\n",
    "        self.lightning_model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "    def prepare_data(self, df: pd.DataFrame, \n",
    "                    user_col: str = 'user_id',\n",
    "                    item_col: str = 'item_id',\n",
    "                    rating_col: Optional[str] = 'rating',\n",
    "                    time_col: Optional[str] = 'timestamp',\n",
    "                    additional_features: Optional[List[str]] = None,\n",
    "                    test_size: float = 0.2,\n",
    "                    val_size: float = 0.1):\n",
    "        \"\"\"Подготовка данных с энкодингом и разделением\"\"\"\n",
    "        \n",
    "        # Создаем энкодеры\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        \n",
    "        df[user_col] = self.user_encoder.fit_transform(df[user_col])\n",
    "        df[item_col] = self.item_encoder.fit_transform(df[item_col])\n",
    "        \n",
    "        # Разделение данных\n",
    "        if self.task_type == 'sequential':\n",
    "            # Для последовательных задач разбиваем по времени\n",
    "            df = df.sort_values([user_col, time_col])\n",
    "            user_groups = df.groupby(user_col)\n",
    "            train_data, test_data = [], []\n",
    "            \n",
    "            for user_id, user_df in user_groups:\n",
    "                n_interactions = len(user_df)\n",
    "                split_idx = int(n_interactions * (1 - test_size))\n",
    "                train_data.append(user_df.iloc[:split_idx])\n",
    "                test_data.append(user_df.iloc[split_idx:])\n",
    "            \n",
    "            train_df = pd.concat(train_data)\n",
    "            test_df = pd.concat(test_data)\n",
    "            \n",
    "        else:\n",
    "            # Для других задач случайное разделение\n",
    "            train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "        \n",
    "        if len(train_df) > len(test_df):\n",
    "            train_df, val_df = train_test_split(train_df, test_size=val_size/(1-test_size), random_state=42)\n",
    "        else:\n",
    "            val_df = test_df\n",
    "        \n",
    "        # Создание датасетов\n",
    "        train_dataset = UniversalRecDataset(\n",
    "            train_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type, additional_features\n",
    "        )\n",
    "        \n",
    "        val_dataset = UniversalRecDataset(\n",
    "            val_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type, additional_features\n",
    "        )\n",
    "        \n",
    "        test_dataset = UniversalRecDataset(\n",
    "            test_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type, additional_features\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def build_model(self, n_users: int, n_items: int):\n",
    "        \"\"\"Создание модели\"\"\"\n",
    "        if self.model_type == 'sasrec':\n",
    "            self.model = SASRec(n_items, self.embedding_dim, self.n_heads, self.n_layers)\n",
    "        elif self.model_type == 'deepfm':\n",
    "            self.model = DeepFM(n_users, n_items, self.embedding_dim)\n",
    "        elif self.model_type == 'wide_deep':\n",
    "            self.model = WideDeep(n_users, n_items, self.embedding_dim)\n",
    "        elif self.model_type == 'lightgcn':\n",
    "            self.model = LightGCN(n_users, n_items, self.embedding_dim, self.n_layers)\n",
    "        elif self.model_type == 'advanced_seq':\n",
    "            self.model = AdvancedSequentialModel(\n",
    "                n_items, self.embedding_dim, self.n_heads, self.n_layers\n",
    "            )\n",
    "        elif self.model_type == 'universal':\n",
    "            self.model = UniversalPowerfulModel(\n",
    "                n_users, n_items, self.model_types, self.embedding_dim, \n",
    "                self.hidden_dim, self.n_heads, self.n_layers, self.sequence_length\n",
    "            )\n",
    "        else:  # 'mf'\n",
    "            self.model = UniversalPowerfulModel(\n",
    "                n_users, n_items, [self.model_type], self.embedding_dim, \n",
    "                self.hidden_dim, self.n_heads, self.n_layers, self.sequence_length\n",
    "            )\n",
    "        \n",
    "        self.lightning_model = UniversalRecLightning(\n",
    "            model=self.model,\n",
    "            task_type=self.task_type,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, num_workers: int = 4):\n",
    "        \"\"\"Обучение модели\"\"\"\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_loss' if self.task_type == 'classic' else 'val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            filename='best-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "        \n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Создание трейнера\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            accelerator='auto',\n",
    "            devices='auto',\n",
    "            precision=16 if torch.cuda.is_available() else 32,\n",
    "            gradient_clip_val=1.0,\n",
    "            accumulate_grad_batches=2  # Для стабильности обучения\n",
    "        )\n",
    "        \n",
    "        # Обучение\n",
    "        self.trainer.fit(\n",
    "            self.lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Обучение завершено. Лучшая модель: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "    def predict_top_k(self, user_ids: List[int], k: int = 20) -> np.ndarray:\n",
    "        \"\"\"Генерация top-k рекомендаций для пользователей\"\"\"\n",
    "        self.lightning_model.eval()\n",
    "        \n",
    "        all_items = torch.arange(len(self.item_encoder.classes_))\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for user_id in user_ids:\n",
    "                user_tensor = torch.LongTensor([user_id])\n",
    "                \n",
    "                # Для sequential моделей нужно сгенерировать последовательность\n",
    "                # В реальности используйте исторические данные пользователя\n",
    "                if hasattr(self.model, 'forward') and any(\n",
    "                    mt in ['sasrec', 'advanced_seq'] for mt in self.model.model_types\n",
    "                ):\n",
    "                    # Для sequential моделей в реальности нужно использовать исторические данные\n",
    "                    # Это упрощенная версия\n",
    "                    scores = torch.randn(len(self.item_encoder.classes_))  # Заглушка\n",
    "                else:\n",
    "                    # Для других моделей\n",
    "                    user_items_scores = []\n",
    "                    for item_id in range(len(self.item_encoder.classes_)):\n",
    "                        batch = {\n",
    "                            'user_id': user_tensor,\n",
    "                            'item_id': torch.LongTensor([item_id])\n",
    "                        }\n",
    "                        try:\n",
    "                            score = self.lightning_model.model(batch)\n",
    "                            user_items_scores.append(score.item())\n",
    "                        except:\n",
    "                            user_items_scores.append(0.0)\n",
    "                    \n",
    "                    scores = torch.tensor(user_items_scores)\n",
    "                \n",
    "                # Получаем top-k\n",
    "                top_k = torch.topk(scores, min(k, len(scores)))\n",
    "                predictions.append(top_k.indices.cpu().numpy())\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def generate_submission(self, test_users: List[int], k: int = 20, \n",
    "                           output_file: str = 'submission.csv'):\n",
    "        \"\"\"Генерация submission файла\"\"\"\n",
    "        predictions = self.predict_top_k(test_users, k)\n",
    "        \n",
    "        # Декодируем ID обратно\n",
    "        decoded_predictions = []\n",
    "        for pred in predictions:\n",
    "            decoded = self.item_encoder.inverse_transform(pred)\n",
    "            decoded_predictions.append(' '.join(map(str, decoded)))\n",
    "        \n",
    "        submission_df = pd.DataFrame({\n",
    "            'user_id': self.user_encoder.inverse_transform(test_users),\n",
    "            'item_ids': decoded_predictions\n",
    "        })\n",
    "        \n",
    "        submission_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Submission сохранен в {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def example_powerful_t_shopping():\n",
    "    \"\"\"Пример для T-Shopping с мощной моделью\"\"\"\n",
    "    # df = pd.read_parquet('train_data.pq')\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='advanced_seq',  # или 'sasrec' для более простой версии\n",
    "        task_type='sequential',\n",
    "        embedding_dim=256,\n",
    "        hidden_dim=512,\n",
    "        n_heads=8,\n",
    "        n_layers=3,\n",
    "        sequence_length=100,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=64,  # Уменьшите если памяти не хватает\n",
    "        max_epochs=30\n",
    "    )\n",
    "    \n",
    "    # Подготовка данных\n",
    "    # train_dataset, val_dataset, test_dataset = trainer.prepare_data(\n",
    "    #     df, user_col='user_id', item_col='item_id', time_col='date'\n",
    "    # )\n",
    "    \n",
    "    # Создание модели\n",
    "    # trainer.build_model(\n",
    "    #     n_users=len(trainer.user_encoder.classes_),\n",
    "    #     n_items=len(trainer.item_encoder.classes_)\n",
    "    # )\n",
    "    \n",
    "    # Обучение\n",
    "    # trainer.train(train_dataset, val_dataset)\n",
    "    \n",
    "    # Генерация submission\n",
    "    # test_users = list(set(df['user_id']))\n",
    "    # trainer.generate_submission(test_users, k=20)\n",
    "\n",
    "def example_universal_model():\n",
    "    \"\"\"Пример универсальной модели, комбинирующей разные подходы\"\"\"\n",
    "    # df = pd.read_csv('ratings.csv')\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='universal',\n",
    "        model_types=['mf', 'deepfm', 'wide_deep'],  # Комбинируем разные подходы\n",
    "        task_type='classic',\n",
    "        embedding_dim=128,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "    \n",
    "    # Аналогично: prepare_data -> build_model -> train\n",
    "\n",
    "def example_graph_model():\n",
    "    \"\"\"Пример с LightGCN (графовая модель)\"\"\"\n",
    "    # df = pd.read_csv('interactions.csv')\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='lightgcn',\n",
    "        task_type='implicit',\n",
    "        embedding_dim=64,\n",
    "        n_layers=3,\n",
    "        learning_rate=1e-2  # Для LightGCN часто используется более высокий LR\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "МОЩНЫЕ МОДЕЛИ ПО ЗАДАЧАМ:\n",
    "\n",
    "1. T-SHOPPING (Next Item Prediction):\n",
    "   - 'sasrec': Transformer-based, excellent for sequences\n",
    "   - 'advanced_seq': Custom attention with multiple heads\n",
    "   - embedding_dim: 256-512\n",
    "   - n_heads: 8-16\n",
    "   - n_layers: 2-4\n",
    "\n",
    "2. LARGE-SCALE IMPLICIT:\n",
    "   - 'lightgcn': Graph-based, scales well\n",
    "   - 'deepfm': Combines FM and deep learning\n",
    "   - embedding_dim: 64-256\n",
    "\n",
    "3. CLASSIC RATING PREDICTION:\n",
    "   - 'deepfm': Best for feature-rich data\n",
    "   - 'wide_deep': Good for sparse features\n",
    "   - 'universal': Combines multiple approaches\n",
    "\n",
    "4. HYBRID TASKS:\n",
    "   - 'universal': Combines multiple model types\n",
    "   - model_types: ['mf', 'deepfm', 'sasrec'] for maximum power\n",
    "\n",
    "НАСТРОЙКИ ДЛЯ МАКСИМАЛЬНОЙ МОЩИ:\n",
    "- embedding_dim: 256, 512 (для сложных паттернов)\n",
    "- n_heads: 8, 16 (для attention моделей)\n",
    "- n_layers: 3, 4, 6 (для глубоких моделей)\n",
    "- hidden_dim: 512, 1024 (для deep parts)\n",
    "- sequence_length: 100, 200 (для long sequences)\n",
    "- batch_size: 32, 64 (адаптируйте под память GPU)\n",
    "\n",
    "ПАМЯТЬ И ПРОИЗВОДИТЕЛЬНОСТЬ:\n",
    "- Используйте gradient accumulation для больших моделей\n",
    "- Включите mixed precision training (precision=16)\n",
    "- Используйте model parallelism для очень больших моделей\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
