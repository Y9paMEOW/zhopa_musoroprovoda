{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Гайд по составлению промптов — правила и шаблоны (принципы + примеры)\n",
    "\n",
    "Основные правила (коротко):\n",
    "\n",
    "Всегда начинай с чёткого SYSTEM / ROLE: кто модель и как себя ведёт.\n",
    "\n",
    "Дальше — цель: что требуется сделать (one-line).\n",
    "\n",
    "Ограничения/формат: «верни только JSON», «ответ в одну строку», «макс N слов».\n",
    "\n",
    "Примеры (few-shot) — 2–3 примера разбора.\n",
    "\n",
    "Postconditions: как валидировать/нормализовать вывод (например, lowercase, trim spaces).\n",
    "\n",
    "Выставь параметры генерации: temperature=0 (deterministic) для точных задач; >0 для креатива.\n",
    "\n",
    "Для сложного reasoning — проси CoT, но в конце требуй итог в маркере <<<ANS>>>.\n",
    "\n",
    "Структура промпта (template):\n",
    "\n",
    "SYSTEM: <role + behavior — e.g., \"You are a concise JSON generator.\">\n",
    "INSTRUCTION: <what to do — single sentence>\n",
    "\n",
    "CONSTRAINTS:\n",
    "- <format constraints>\n",
    "- <content constraints>\n",
    "- <max_length>\n",
    "\n",
    "EXAMPLES:\n",
    "Input: ...\n",
    "Output: <example exact output>\n",
    "\n",
    "NOW:\n",
    "Input: <actual input>\n",
    "Answer:\n",
    "\n",
    "\n",
    "Пример: сделать краткое содержание (English):\n",
    "\n",
    "SYSTEM: You are a professional summarizer. Return only JSON.\n",
    "INSTRUCTION: Summarize the input into at most 2 sentences.\n",
    "\n",
    "SCHEMA: {\"id\":\"string\",\"summary\":\"string\"}\n",
    "\n",
    "EXAMPLE\n",
    "Input: Article about X...\n",
    "Output: {\"id\":\"a1\",\"summary\":\"...\"}\n",
    "---\n",
    "Now Input: <article>\n",
    "Output:\n",
    "\n",
    "\n",
    "Промпт для Chain-of-Thought + final answer marker:\n",
    "\n",
    "SYSTEM: You are a helpful reasoning assistant.\n",
    "TASK: Solve the problem step-by-step, then give final answer after <<<ANSWER>>> marker.\n",
    "Question: ...\n",
    "Think step-by-step:\n",
    "1) ...\n",
    "2) ...\n",
    "<<<ANSWER>>> Final answer: ...\n",
    "\n",
    "\n",
    "Если judge требует только финал — парсите текст после <<<ANSWER>>>.\n",
    "\n",
    "Пара советов для RAG & retrieval:\n",
    "\n",
    "В prompt включай: query + top_k retrieved docs (each labeled [1], [2]...) + instruction Use only the context to answer.\n",
    "\n",
    "Ставьте explicit contradiction rule: «If no evidence in context, answer NO_ANSWER».\n",
    "\n",
    "Параметры модели:\n",
    "\n",
    "deterministic tasks → temperature=0, top_p=1, num_beams>1 (если модель поддерживает), max_new_tokens appropriate.\n",
    "\n",
    "generation for creativity → temperature 0.7—1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Генерация и формирование JSON через LLM — что нужно делать, чеклист и шаблоны\n",
    "\n",
    "Что организатор может требовать: ровно валидный JSON-файл, строгое поле/типовое соответствие (строки/числа/списки), иногда сортировка/вложенность. Основная проблема — LLM склонна «добавлять» объяснения, кавычки/комментарии, или выдавать невалидный JSON.\n",
    "\n",
    "Чеклист для гарантии валидного JSON:\n",
    "\n",
    "Заранее описать строгую схему (поля, типы, допустимые значения).\n",
    "\n",
    "В prompt попросить только JSON, без лишнего текста, и поставить стоп-токен(ы).\n",
    "\n",
    "Привести 2—3 коротких few-shot примера (вход → корректный JSON).\n",
    "\n",
    "Принимать вывод LLM и валидировать локально (json.loads / jsonschema).\n",
    "\n",
    "Если невалидный — использовать автоматическую «repair prompt»: передать LLM исходный вывод и попросить вернуть исправленный JSON (или использовать jsonrepair heuristics).\n",
    "\n",
    "Для надёжности: запускать jsonschema-валидацию и нормализацию типов (например, числа из строк).\n",
    "\n",
    "Минимальный шаблон system+user для генерации JSON (англ):\n",
    "\n",
    "SYSTEM: You are a JSON-output generator. Always answer **only** a single JSON object and nothing else.\n",
    "\n",
    "USER:\n",
    "Input: <here put original text or question>\n",
    "\n",
    "Schema:\n",
    "{\n",
    "  \"id\": \"string\",\n",
    "  \"summary\": \"string (max 300 chars)\",\n",
    "  \"tags\": [\"string\"],\n",
    "  \"score\": \"number (0-1)\"\n",
    "}\n",
    "\n",
    "Return value: exactly one JSON object that matches the schema above. Use the following examples:\n",
    "\n",
    "Example 1:\n",
    "Input: \"...\"\n",
    "Output: {\"id\":\"doc1\",\"summary\":\"...\",\"tags\":[\"a\",\"b\"],\"score\":0.73}\n",
    "\n",
    "After examples: Now produce JSON for Input: <...>\n",
    "\n",
    "\n",
    "Русский вариант (строго):\n",
    "\n",
    "SYSTEM: Ты — генератор, отвечай строго одним JSON-объектом. Никаких комментариев, заголовков или лишнего текста.\n",
    "\n",
    "Пользователь:\n",
    "Текст: <...>\n",
    "\n",
    "Схема:\n",
    "{\n",
    "  \"id\": \"строка\",\n",
    "  \"answer\": \"строка (макс 200 символов)\",\n",
    "  \"labels\": [\"строка\"],\n",
    "  \"confidence\": \"число от 0 до 1\"\n",
    "}\n",
    "\n",
    "Верни ровно один JSON, корректный по синтаксису.\n",
    "\n",
    "\n",
    "Stop-sequences / параметры:\n",
    "\n",
    "temperature = 0.0 (детерминированно), top_p small (0.8), max_new_tokens = небольшой лимит(<=512).\n",
    "\n",
    "Указывайте stop на \\n\\n или </s> если модель поддерживает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunck example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def split_into_sentences_regex(text: str) -> List[str]:\n",
    "    sentences = re.split(r'(?<=[.!?。！？])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "class RegexChunker:\n",
    "    def __init__(self, tokenizer_name=\"gpt2\", max_tokens=2048, overlap_tokens=200, min_tokens=40):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.min_tokens = min_tokens\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[dict]:\n",
    "        sentences = split_into_sentences_regex(text)\n",
    "        chunks = []\n",
    "        cur_sent = []\n",
    "        cur_tokens = 0\n",
    "        for sent in sentences:\n",
    "            tlen = self.count_tokens(sent)\n",
    "            if cur_tokens + tlen <= self.max_tokens:\n",
    "                cur_sent.append(sent)\n",
    "                cur_tokens += tlen\n",
    "            else:\n",
    "                if cur_sent:\n",
    "                    chunks.append({\"text\": \" \".join(cur_sent), \"tokens\": cur_tokens})\n",
    "                # create carryover for overlap: take last sentences until overlap_tokens satisfied\n",
    "                carry = []\n",
    "                carry_tokens = 0\n",
    "                i = len(cur_sent) - 1\n",
    "                while i >= 0 and carry_tokens < self.overlap_tokens:\n",
    "                    s = cur_sent[i]\n",
    "                    carry.insert(0, s)\n",
    "                    carry_tokens += self.count_tokens(s)\n",
    "                    i -= 1\n",
    "                cur_sent = carry + [sent]\n",
    "                cur_tokens = carry_tokens + tlen\n",
    "        if cur_sent:\n",
    "            chunks.append({\"text\": \" \".join(cur_sent), \"tokens\": cur_tokens})\n",
    "\n",
    "        # postprocess: merge too-short chunks with neighbor\n",
    "        final = []\n",
    "        for ch in chunks:\n",
    "            if final and ch[\"tokens\"] < self.min_tokens:\n",
    "                final[-1][\"text\"] += \" \" + ch[\"text\"]\n",
    "                final[-1][\"tokens\"] += ch[\"tokens\"]\n",
    "            else:\n",
    "                final.append(ch)\n",
    "        # ensure that none exceed max_tokens (handle long single sentences)\n",
    "        for i, ch in enumerate(final):\n",
    "            if ch[\"tokens\"] > self.max_tokens:\n",
    "                tokens = ch[\"text\"].split()\n",
    "                approx_chunk_size = max(1, self.max_tokens // 2)\n",
    "                new_chunks = []\n",
    "                start = 0\n",
    "                while start < len(tokens):\n",
    "                    part = \" \".join(tokens[start:start + approx_chunk_size])\n",
    "                    new_chunks.append({\"text\": part, \"tokens\": self.count_tokens(part)})\n",
    "                    start += approx_chunk_size\n",
    "                final[i:i+1] = new_chunks\n",
    "        return final\n",
    "\n",
    "# Example usage:\n",
    "# chunker = RegexChunker(tokenizer_name=\"gpt2\", max_tokens=1024, overlap_tokens=128, min_tokens=30)\n",
    "# chunks = chunker.chunk_text(large_text)\n",
    "# for c in chunks: print(c[\"tokens\"], c[\"text\"][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def split_into_sentences_regex(text: str) -> List[str]:\n",
    "    sentences = re.split(r'(?<=[.!?。！？])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "class LLMChunker:\n",
    "    def __init__(self,\n",
    "                 tokenizer_name_for_count=\"gpt2\",\n",
    "                 llm_model_name=\"google/flan-t5-small\",\n",
    "                 max_tokens=2048,\n",
    "                 overlap_tokens=200,\n",
    "                 candidate_window=8,\n",
    "                 device=None):\n",
    "        self.count_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_for_count, use_fast=True)\n",
    "        self.llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name, use_fast=True)\n",
    "        self.llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm_model.to(self.device)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.candidate_window = candidate_window\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        return len(self.count_tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    def ask_llm_choose_boundary(self, candidates: List[str]) -> int:\n",
    "        # candidates is a list of candidate sentence texts (short), indexed 1..n\n",
    "        # prompt asks to return integer index (1-based) of best boundary (only the index)\n",
    "        prompt_parts = []\n",
    "        prompt_parts.append(\"You are a helpful assistant that chooses the best boundary to split a text chunk.\")\n",
    "        prompt_parts.append(\"Given the following candidate sentence endings, choose the index (1-based) \"\n",
    "                            \"which is the best place to cut so that the first part is a complete, self-contained chunk.\")\n",
    "        prompt_parts.append(\"Answer with a single integer index from the list of options. Do not output extra text.\")\n",
    "        prompt_parts.append(\"Candidates:\")\n",
    "        for i, s in enumerate(candidates, 1):\n",
    "            txt = s.replace(\"\\n\", \" \").strip()\n",
    "            prompt_parts.append(f\"{i}: {txt}\")\n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.llm_model.generate(**inputs, max_new_tokens=8, do_sample=False)\n",
    "        resp = self.llm_tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
    "        # parse integer\n",
    "        for token in resp.split():\n",
    "            if token.isdigit():\n",
    "                idx = int(token)\n",
    "                if 1 <= idx <= len(candidates):\n",
    "                    return idx\n",
    "        # fallback: choose middle\n",
    "        return max(1, len(candidates)//2)\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[dict]:\n",
    "        sents = split_into_sentences_regex(text)\n",
    "        chunks = []\n",
    "        cur = []\n",
    "        cur_tokens = 0\n",
    "        i = 0\n",
    "        N = len(sents)\n",
    "        pbar = tqdm(total=N, desc=\"LLMChunker\")\n",
    "        while i < N:\n",
    "            sent = sents[i]\n",
    "            tlen = self.count_tokens(sent)\n",
    "            if cur_tokens + tlen <= self.max_tokens:\n",
    "                cur.append(sent)\n",
    "                cur_tokens += tlen\n",
    "                i += 1\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                # need to select a boundary within last candidate_window sentences of cur\n",
    "                window_start = max(0, len(cur) - self.candidate_window)\n",
    "                candidates = cur[window_start:]  # sentences that can be chosen as last in chunk\n",
    "                # ensure tokens count for each candidate prefix not exceeding max: build prefix lists\n",
    "                prefix_texts = []\n",
    "                total = 0\n",
    "                # create candidate prefixes (from window_start..end)\n",
    "                for j in range(window_start, len(cur)):\n",
    "                    prefix = \" \".join(cur[:j+1])\n",
    "                    if self.count_tokens(prefix) <= self.max_tokens:\n",
    "                        prefix_texts.append(\" \".join(cur[j: j+1]))  # candidate sentence text\n",
    "                if not prefix_texts:\n",
    "                    # fallback: hard cut max_tokens worth of last tokens\n",
    "                    chunks.append({\"text\": \" \".join(cur), \"tokens\": cur_tokens})\n",
    "                    cur = []\n",
    "                    cur_tokens = 0\n",
    "                    continue\n",
    "                chosen_local_index = self.ask_llm_choose_boundary(prefix_texts)\n",
    "                # map chosen_local_index to absolute position\n",
    "                chosen_sent_index = window_start + (chosen_local_index - 1)\n",
    "                # form chunk up to chosen_sent_index\n",
    "                chunk_sentences = cur[:chosen_sent_index+1]\n",
    "                chunk_text = \" \".join(chunk_sentences)\n",
    "                chunk_tokens = self.count_tokens(chunk_text)\n",
    "                chunks.append({\"text\": chunk_text, \"tokens\": chunk_tokens})\n",
    "                # prepare overlap: carry last overlap_tokens worth of sentences\n",
    "                carry = []\n",
    "                carry_tokens = 0\n",
    "                k = len(cur) - 1\n",
    "                while k > chosen_sent_index and carry_tokens < self.overlap_tokens:\n",
    "                    scarry = cur[k]\n",
    "                    carry.insert(0, scarry)\n",
    "                    carry_tokens += self.count_tokens(scarry)\n",
    "                    k -= 1\n",
    "                cur = carry\n",
    "                cur_tokens = carry_tokens\n",
    "        # end loop\n",
    "        if cur:\n",
    "            chunks.append({\"text\": \" \".join(cur), \"tokens\": cur_tokens})\n",
    "        pbar.close()\n",
    "        # merge small chunks\n",
    "        final = []\n",
    "        for ch in chunks:\n",
    "            if final and ch[\"tokens\"] < max(40, int(0.05 * self.max_tokens)):\n",
    "                final[-1][\"text\"] += \" \" + ch[\"text\"]\n",
    "                final[-1][\"tokens\"] += ch[\"tokens\"]\n",
    "            else:\n",
    "                final.append(ch)\n",
    "        return final\n",
    "\n",
    "# Example usage:\n",
    "# llm_chunker = LLMChunker(tokenizer_name_for_count=\"gpt2\", llm_model_name=\"google/flan-t5-small\",\n",
    "#                          max_tokens=1024, overlap_tokens=128, candidate_window=6)\n",
    "# chunks = llm_chunker.chunk_text(large_text)\n",
    "# for c in chunks: print(c[\"tokens\"], c[\"text\"][:120])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендации (по сценарию)\n",
    "\n",
    "LLM-assist (быстрый, дешёвый, детерминированный) — google/flan-t5-small или google/flan-t5-base.\n",
    "Почему: очень быстрые, дешёвые в inference, хорошо подходят для «выбора границы» при LLM-assisted chunking (prompt → вернуть индекс). Они сохраняют разумное «понимание» семантики при малом latency. (если нужен максимум качества — взять flan-t5-large/XL). \n",
    "graphcore.ai\n",
    "\n",
    "LLM-assist (лучшее качество, можно пожертвовать скоростью) — Mistral-7B-Instruct, Llama-2-7B-chat (или аналогичные 7B-инструкционные модели).\n",
    "Почему: значительно богаче семантически, точнее выбирают естественные границы, особенно на сложных текстах; подходят если у вас A100 и вы готовы платить вёртким latency и памяти. \n",
    "arXiv\n",
    "\n",
    "Если нужно мульти-язык (RU+EN) у LLM-assist — flan-t5 (многоязычные варианты) или специально русские/мультиязычные инстр.-модели (Llama3.1-8B Instruct / русифицированные 7B). При отсутствии хорошей русскоязычной версии — делайте трансляцию (RU→EN chunking ask → EN→RU post-translation). \n",
    "graphcore.ai\n",
    "+1\n",
    "\n",
    "Для семантических решений / embedding-основы (semantic chunking, кластеринг, merge/score) — sentence-transformers/all-mpnet-base-v2 (качество) и sentence-transformers/all-MiniLM-L6-v2 (скорость / экономия). Для multilingual — paraphrase-multilingual-MiniLM-L12-v2 или distiluse-base-multilingual-cased-v1. Эти модели — de-facto стандарт для embedding-based semantic splitting и retrieval. \n",
    "sbert.net\n",
    "+1\n",
    "\n",
    "Если хотите state-of-the-art «chunking as LLM task» метод — берите компактный Flan-T5 (small/base) и используйте схему, похожую на PIC / LLM-guided chunking (см. ACL 2025 paper — LLM генерирует pseudo-instructions / выбирает границы) — это даёт хорошее соотношение цена/качество. \n",
    "aclanthology.org\n",
    "\n",
    "Общие правила\n",
    "\n",
    "Для LLM-assist ставьте temperature=0.0 и do_sample=False — вы хотите детерминированный, стабильный выбор границы.\n",
    "\n",
    "Делайте candidate_window (6–12 предложений) вместо опроса всех возможных мест — экономит запросы и фокусирует модель. (техника проверена в recent work «LLM-guided segmentation / PIC»). \n",
    "aclanthology.org\n",
    "\n",
    "Для embeddings: сначала пробуйте all-MiniLM-L6-v2 (скорость) и all-mpnet-base-v2 для финальной оценки/сравнения. Если вам важен RU — paraphrase-multilingual-MiniLM-L12-v2. \n",
    "\n",
    "Примеры рабочих комбинаций (быстрые рецепты)\n",
    "\n",
    "Speed-first: flan-t5-small (boundary choice) + all-MiniLM-L6-v2 (embeddings) → дешёво, быстро.\n",
    "\n",
    "Quality-first: mistral-7b-instruct (boundary) + all-mpnet-base-v2 (embeddings) + overlap=10% → лучшие разделы, чуть дороже.\n",
    "\n",
    "Multilingual: flan-t5-base or multilingual Flan variant (boundary) + paraphrase-multilingual-MiniLM-L12-v2 (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаблон промптов\n",
    "\n",
    "EN prompt (строгий, информативный):\n",
    "\n",
    "Describe the image in 1–2 concise sentences (max ~30 words). Mention the main objects, their attributes (color, size), and any visible action or scene context. Do not guess identities, dates, or text in the image. Use present tense. Return only the caption, nothing else.\n",
    "\n",
    "\n",
    "RU prompt:\n",
    "\n",
    "Опиши изображение в 1–2 коротких предложениях (макс ~30 слов). Укажи основные объекты, их признаки (цвет, размер), действие и контекст сцены. Не придумывай имён, дат или текстов. Используй настоящее время. Верни только подпись — ничего лишнего.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "INPUT_CSV = \"images.csv\"         # входной CSV\n",
    "IMAGE_COL = \"image_path\"         # колонка с путями к изображениям\n",
    "OUTPUT_CSV = \"images_with_captions.csv\"\n",
    "MODEL_NAME = \"Salesforce/blip2-opt-2.7b\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_NEW_TOKENS = 64\n",
    "NUM_BEAMS = 4\n",
    "LANG = \"en\"                      # \"en\" или \"ru\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "prompt_map = {\n",
    "    \"en\": \"Describe the image in 1–2 concise sentences (max ~30 words). Mention the main objects, their attributes (color, size), and any visible action or scene context. Do not guess identities, dates, or text in the image. Use present tense. Return only the caption, nothing else.\",\n",
    "    \"ru\": \"Опиши изображение в 1–2 коротких предложениях (макс ~30 слов). Укажи основные объекты, их признаки (цвет, размер), действие и контекст сцены. Не придумывай имён, дат или текстов. Используй настоящее время. Верни только подпись — ничего лишнего.\"\n",
    "}\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "paths = df[IMAGE_COL].astype(str).tolist()\n",
    "captions = []\n",
    "\n",
    "def load_image(p):\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "batch_images = []\n",
    "batch_idxs = []\n",
    "pbar = tqdm(range(0, len(paths), BATCH_SIZE), desc=\"Batches\")\n",
    "for i in pbar:\n",
    "    batch_paths = paths[i:i+BATCH_SIZE]\n",
    "    imgs = [load_image(p) for p in batch_paths]\n",
    "    text_prompt = prompt_map.get(LANG, prompt_map[\"en\"])\n",
    "    inputs = processor(images=imgs, text=text_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            num_beams=NUM_BEAMS,\n",
    "            do_sample=False,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    decoded = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded = [d.strip() for d in decoded]\n",
    "    captions.extend(decoded)\n",
    "\n",
    "if len(captions) < len(paths):\n",
    "    captions.extend([\"\"] * (len(paths) - len(captions)))\n",
    "\n",
    "df[\"caption\"] = captions\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sobz-dev/VLM_Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization model 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # пример; замените на любую поддерживаемую модель\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",        # автоматическое распределение между GPU/CPU\n",
    "    load_in_8bit=True,        # включаем 8-bit (bitsandbytes)\n",
    "    torch_dtype=torch.float16 # вычисления делаем в fp16 на GPU\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Summarize the following text in one sentence: 'Transformers are a family of neural networks...'\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=60, do_sample=False, num_beams=3)\n",
    "    out = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization model 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"  # пример; замените\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # dtype для вычислений\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NF4 quantization (лучше качество)\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Explain in one sentence why the sky is blue.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    out_ids = model.generate(**inputs, max_new_tokens=80, do_sample=False)\n",
    "    print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
