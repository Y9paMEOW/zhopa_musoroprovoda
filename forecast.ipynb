{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826923b7",
   "metadata": {},
   "source": [
    "DLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Диссекция DLinear: \n",
    "    - Универсальная модель для univariate и multivariate задач.\n",
    "    - Поддерживает флаг --individual (для мультивариантной задачи с независимыми линейными слоями).\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len: int, pred_len: int, individual: bool = False, n_vars: int = 1):\n",
    "        \"\"\"\n",
    "        seq_len: длина входной последовательности\n",
    "        pred_len: длина прогноза\n",
    "        individual: если True, для каждого признака обучается отдельный линейный слой (аналог --individual в официальном коде)\n",
    "        n_vars: количество переменных (признаков), используется при individual=True\n",
    "        \"\"\"\n",
    "        super(DLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.output_dim = pred_len\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        # Усреднение по времени (для тренда)\n",
    "        self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n",
    "        self.Linear_Trend = nn.Linear(seq_len, pred_len)\n",
    "\n",
    "        if self.individual:\n",
    "            # Для каждого признака — отдельный линейный слой\n",
    "            self.Linear_Seasonal.weight = nn.Parameter(torch.zeros(n_vars, pred_len, seq_len))\n",
    "            self.Linear_Seasonal.bias = nn.Parameter(torch.zeros(n_vars, pred_len))\n",
    "            self.Linear_Trend.weight = nn.Parameter(torch.zeros(n_vars, pred_len, seq_len))\n",
    "            self.Linear_Trend.bias = nn.Parameter(torch.zeros(n_vars, pred_len))\n",
    "        else:\n",
    "            # Общий линейный слой для всех признаков\n",
    "            self.Linear_Seasonal.weight = nn.Parameter(torch.zeros(pred_len, seq_len))\n",
    "            self.Linear_Seasonal.bias = nn.Parameter(torch.zeros(pred_len))\n",
    "            self.Linear_Trend.weight = nn.Parameter(torch.zeros(pred_len, seq_len))\n",
    "            self.Linear_Trend.bias = nn.Parameter(torch.zeros(pred_len))\n",
    "\n",
    "        # Инициализация весов\n",
    "        nn.init.kaiming_uniform_(self.Linear_Seasonal.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.Linear_Trend.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, n_vars)\n",
    "        return: (batch_size, pred_len, n_vars)\n",
    "        \"\"\"\n",
    "        # Декомпозиция: вычитание среднего (тренда)\n",
    "        seasonal_init = x\n",
    "        trend = seasonal_init.mean(dim=1, keepdim=True)\n",
    "        seasonal = seasonal_init - trend\n",
    "\n",
    "        # Транспонируем для линейного слоя: (batch, n_vars, seq_len)\n",
    "        seasonal = seasonal.transpose(1, 2)  # (b, n_vars, seq_len)\n",
    "        trend = trend.transpose(1, 2)       # (b, n_vars, seq_len)\n",
    "\n",
    "        if self.individual:\n",
    "            # Применяем линейные слои индивидуально для каждого признака\n",
    "            seasonal_output = torch.stack([\n",
    "                self.Linear_Seasonal[i](seasonal[:, i, :]) for i in range(self.n_vars)\n",
    "            ], dim=1)  # (b, n_vars, pred_len)\n",
    "\n",
    "            trend_output = torch.stack([\n",
    "                self.Linear_Trend[i](trend[:, i, :]) for i in range(self.n_vars)\n",
    "            ], dim=1)  # (b, n_vars, pred_len)\n",
    "        else:\n",
    "            # Применяем один линейный слой ко всем признакам\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal)  # (b, n_vars, pred_len)\n",
    "            trend_output = self.Linear_Trend(trend)          # (b, n_vars, pred_len)\n",
    "\n",
    "        # Складываем сезонность и тренд\n",
    "        x = seasonal_output + trend_output\n",
    "        # Транспонируем обратно: (batch, pred_len, n_vars)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Параметры задачи\n",
    "    seq_len = 96\n",
    "    pred_len = 48\n",
    "    n_vars = 7  # например, 7 признаков\n",
    "    batch_size = 32\n",
    "\n",
    "    # --- Univariate ---\n",
    "    print(\"=== Univariate ===\")\n",
    "    model_uni = DLinear(seq_len=seq_len, pred_len=pred_len, individual=False, n_vars=1)\n",
    "    x_uni = torch.randn(batch_size, seq_len, 1)\n",
    "    out_uni = model_uni(x_uni)\n",
    "    print(f\"Input: {x_uni.shape} -> Output: {out_uni.shape}\")\n",
    "\n",
    "    # --- Multivariate (non-individual) ---\n",
    "    print(\"\\n=== Multivariate (non-individual) ===\")\n",
    "    model_multi = DLinear(seq_len=seq_len, pred_len=pred_len, individual=False, n_vars=n_vars)\n",
    "    x_multi = torch.randn(batch_size, seq_len, n_vars)\n",
    "    out_multi = model_multi(x_multi)\n",
    "    print(f\"Input: {x_multi.shape} -> Output: {out_multi.shape}\")\n",
    "\n",
    "    # --- Multivariate (individual) ---\n",
    "    print(\"\\n=== Multivariate (individual) ===\")\n",
    "    model_indiv = DLinear(seq_len=seq_len, pred_len=pred_len, individual=True, n_vars=n_vars)\n",
    "    x_indiv = torch.randn(batch_size, seq_len, n_vars)\n",
    "    out_indiv = model_indiv(x_indiv)\n",
    "    print(f\"Input: {x_indiv.shape} -> Output: {out_indiv.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8eb8e",
   "metadata": {},
   "source": [
    "NLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126053af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Non-stationary Linear Model (NLinear)\n",
    "    Основано на статье: https://arxiv.org/abs/2205.13504\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len: int, pred_len: int, individual: bool = False, n_vars: int = 1):\n",
    "        \"\"\"\n",
    "        seq_len: длина входной последовательности\n",
    "        pred_len: длина прогноза\n",
    "        individual: если True, для каждого признака обучается отдельный линейный слой\n",
    "        n_vars: количество переменных (признаков)\n",
    "        \"\"\"\n",
    "        super(NLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear = nn.ModuleList()\n",
    "            for i in range(n_vars):\n",
    "                self.Linear.append(nn.Linear(seq_len, pred_len))\n",
    "        else:\n",
    "            self.Linear = nn.Linear(seq_len, pred_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, n_vars)\n",
    "        return: (batch_size, pred_len, n_vars)\n",
    "        \"\"\"\n",
    "        if self.individual:\n",
    "            # Применяем линейный слой к каждому признаку отдельно\n",
    "            output = torch.stack(\n",
    "                [self.Linear[i](x[:, :, i]) for i in range(self.n_vars)], dim=-1\n",
    "            )\n",
    "        else:\n",
    "            # Транспонируем: (b, n_vars, seq_len)\n",
    "            x = x.transpose(1, 2)\n",
    "            output = self.Linear(x)  # (b, n_vars, pred_len)\n",
    "            output = output.transpose(1, 2)  # (b, pred_len, n_vars)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    seq_len = 96\n",
    "    pred_len = 48\n",
    "    n_vars = 7\n",
    "    batch_size = 32\n",
    "\n",
    "    model = NLinear(seq_len=seq_len, pred_len=pred_len, individual=False, n_vars=n_vars)\n",
    "    x = torch.randn(batch_size, seq_len, n_vars)\n",
    "    out = model(x)\n",
    "    print(f\"NLinear | Input: {x.shape} -> Output: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0a27e",
   "metadata": {},
   "source": [
    "PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchTST(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchTST: Transformer with Patching\n",
    "    Основано на статье: https://arxiv.org/abs/2211.14730\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        pred_len: int,\n",
    "        patch_len: int = 16,\n",
    "        stride: int = 8,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 8,\n",
    "        e_layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        individual: bool = False,\n",
    "        n_vars: int = 1\n",
    "    ):\n",
    "        super(PatchTST, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.d_model = d_model\n",
    "        self.n_vars = n_vars\n",
    "        self.individual = individual\n",
    "\n",
    "        # Вычисляем количество патчей\n",
    "        self.num_patches = (seq_len - patch_len) // stride + 1\n",
    "\n",
    "        # Линейный слой для проецирования патчей в d_model\n",
    "        self.patch_projection = nn.Linear(patch_len, d_model)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)\n",
    "\n",
    "        # Предсказание длины pred_len\n",
    "        self.predictor = nn.Linear(d_model, pred_len)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, n_vars)\n",
    "        return: (batch_size, pred_len, n_vars)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_vars = x.shape\n",
    "\n",
    "        # Патчинг: разбиваем на патчи (batch, n_vars, num_patches, patch_len)\n",
    "        patches = x.unfold(dimension=1, size=self.patch_len, step=self.stride)  # (b, n_vars, num_patches, patch_len)\n",
    "        patches = patches.transpose(1, 2)  # (b, num_patches, n_vars, patch_len)\n",
    "\n",
    "        # Проекция патчей в d_model\n",
    "        patches = self.patch_projection(patches)  # (b, num_patches, n_vars, d_model)\n",
    "\n",
    "        # Суммируем признаки вдоль оси n_vars (или используем отдельно, если individual=True)\n",
    "        # Для простоты: усредняем по признакам (в оригинале может быть индивидуальный выход)\n",
    "        patches = patches.mean(dim=2)  # (b, num_patches, d_model)\n",
    "\n",
    "        # Пропускаем через Transformer\n",
    "        out = self.transformer(patches)  # (b, num_patches, d_model)\n",
    "\n",
    "        # Предсказание\n",
    "        out = self.predictor(out)  # (b, num_patches, pred_len)\n",
    "\n",
    "        # Суммируем или усредняем по патчам\n",
    "        out = out.mean(dim=1)  # (b, pred_len)\n",
    "        out = out.unsqueeze(-1).repeat(1, 1, self.n_vars)  # (b, pred_len, n_vars)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    seq_len = 96\n",
    "    pred_len = 48\n",
    "    patch_len = 16\n",
    "    stride = 8\n",
    "    n_vars = 7\n",
    "    batch_size = 32\n",
    "\n",
    "    model = PatchTST(\n",
    "        seq_len=seq_len,\n",
    "        pred_len=pred_len,\n",
    "        patch_len=patch_len,\n",
    "        stride=stride,\n",
    "        n_vars=n_vars\n",
    "    )\n",
    "    x = torch.randn(batch_size, seq_len, n_vars)\n",
    "    out = model(x)\n",
    "    print(f\"PatchTST | Input: {x.shape} -> Output: {out.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
