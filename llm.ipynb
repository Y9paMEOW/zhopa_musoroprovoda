{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss + base RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "INPUT_CSV = \"texts.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"google/flan-t5-small\"\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOP_K = 5\n",
    "OUT_EMB_NPZ = \"embeddings_faiss.npz\"\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL)\n",
    "emb_model = AutoModel.from_pretrained(EMB_MODEL).to(DEVICE)\n",
    "emb_model.eval()\n",
    "\n",
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "all_embs = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"embed\"):\n",
    "        batch_texts = texts[i:i+BATCH_SIZE]\n",
    "        enc = emb_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        out = emb_model(**enc, output_hidden_states=False, return_dict=True)\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            emb = out.pooler_output\n",
    "        else:\n",
    "            emb = mean_pooling(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        emb = emb.cpu().numpy()\n",
    "        all_embs.append(emb)\n",
    "all_embs = np.vstack(all_embs).astype(np.float32)\n",
    "\n",
    "faiss.normalize_L2(all_embs)\n",
    "dim = all_embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(all_embs)\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "np.savez_compressed(OUT_EMB_NPZ, texts=np.array(texts), embeddings=all_embs)\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL).to(DEVICE)\n",
    "llm_model.eval()\n",
    "\n",
    "def retrieve(query, top_k=TOP_K):\n",
    "    q_enc = emb_tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    q_enc = {k: v.to(DEVICE) for k, v in q_enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**q_enc, output_hidden_states=False, return_dict=True)\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            q_emb = out.pooler_output.cpu().numpy()\n",
    "        else:\n",
    "            q_emb = mean_pooling(out.last_hidden_state, q_enc[\"attention_mask\"]).cpu().numpy()\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    ids = I[0].tolist()\n",
    "    scores = D[0].tolist()\n",
    "    retrieved_texts = [texts[idx] for idx in ids]\n",
    "    return retrieved_texts, scores\n",
    "\n",
    "def answer_query(query, retrieved_texts):\n",
    "    context = \"\\n\\n\".join(f\"[{i+1}] {t}\" for i, t in enumerate(retrieved_texts))\n",
    "    prompt = f\"Use the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    enc = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = llm_model.generate(**enc, max_length=256, num_beams=4, early_stopping=True)\n",
    "    return llm_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "example_query = \"What is the main topic of the first document?\"\n",
    "retrieved, scores = retrieve(example_query, top_k=TOP_K)\n",
    "answer = answer_query(example_query, retrieved)\n",
    "print(\"Retrieved:\")\n",
    "for t, s in zip(retrieved, scores):\n",
    "    print(f\"score={s:.4f}\\t{t[:200]}\")\n",
    "print(\"Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss + Reranker + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "INPUT_CSV = \"/kaggle/input/vseros-nlp-qual/train.tsv\"\n",
    "TEXT_COL = \"shortDescription\"\n",
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "RERANKER_MODEL = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "LLM_MODEL = \"google/flan-t5-small\"\n",
    "BATCH_SIZE = 32\n",
    "RERANK_BATCH = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TOP_K = 5\n",
    "OUT_EMB_NPZ = \"embeddings_faiss.npz\"\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV, sep = '\\t')\n",
    "df = df[:100]\n",
    "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMB_MODEL)\n",
    "emb_model = AutoModel.from_pretrained(EMB_MODEL).to(DEVICE)\n",
    "emb_model.eval()\n",
    "\n",
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    summed = (last_hidden_state * mask).sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    return summed / counts\n",
    "\n",
    "all_embs = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"embed\"):\n",
    "        batch_texts = texts[i:i+BATCH_SIZE]\n",
    "        enc = emb_tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        out = emb_model(**enc, output_hidden_states=False, return_dict=True)\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            emb = out.pooler_output\n",
    "        else:\n",
    "            emb = mean_pooling(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        emb = emb.cpu().numpy()\n",
    "        all_embs.append(emb)\n",
    "all_embs = np.vstack(all_embs).astype(np.float32)\n",
    "\n",
    "faiss.normalize_L2(all_embs)\n",
    "dim = all_embs.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(all_embs)\n",
    "faiss.write_index(index, INDEX_FILE)\n",
    "np.savez_compressed(OUT_EMB_NPZ, texts=np.array(texts), embeddings=all_embs)\n",
    "\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL, trust_remote_code=True)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL, trust_remote_code=True).to(DEVICE)\n",
    "if reranker_tokenizer.pad_token is None:\n",
    "    reranker_tokenizer.pad_token = reranker_tokenizer.eos_token\n",
    "reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "reranker_model.eval()\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL).to(DEVICE)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "llm_model.eval()\n",
    "def retrieve(query, top_k=TOP_K):\n",
    "    q_enc = emb_tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    q_enc = {k: v.to(DEVICE) for k, v in q_enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**q_enc, output_hidden_states=False, return_dict=True)\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            q_emb = out.pooler_output.cpu().numpy()\n",
    "        else:\n",
    "            q_emb = mean_pooling(out.last_hidden_state, q_enc[\"attention_mask\"]).cpu().numpy()\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    ids = I[0].tolist()\n",
    "    scores = D[0].tolist()\n",
    "    retrieved_texts = [texts[idx] for idx in ids]\n",
    "    return retrieved_texts, scores, ids\n",
    "\n",
    "def rerank_scores(query, candidates, batch_size=RERANK_BATCH):\n",
    "    pairs = [f\"Query: {query}\\nCandidate: {c}\" for c in candidates]\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch_pairs = pairs[i:i+batch_size]\n",
    "            enc = reranker_tokenizer(batch_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            out = reranker_model(**enc)\n",
    "            logits = out.logits\n",
    "            if logits is None:\n",
    "                batch_scores = [0.0] * len(batch_pairs)\n",
    "            else:\n",
    "                if logits.dim() == 1:\n",
    "                    batch_scores = logits.cpu().numpy().tolist()\n",
    "                    batch_scores = [float(torch.sigmoid(torch.tensor(s)).item()) for s in batch_scores]\n",
    "                elif logits.dim() == 2 and logits.shape[1] == 1:\n",
    "                    batch_scores = logits.squeeze(1).cpu().numpy().tolist()\n",
    "                    batch_scores = [float(torch.sigmoid(torch.tensor(s)).item()) for s in batch_scores]\n",
    "                elif logits.dim() == 2 and logits.shape[1] == 2:\n",
    "                    probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "                    batch_scores = probs.tolist()\n",
    "                else:\n",
    "                    probs = F.softmax(logits, dim=1)[:, -1].cpu().numpy()\n",
    "                    batch_scores = probs.tolist()\n",
    "            scores.extend(batch_scores)\n",
    "    return scores\n",
    "\n",
    "def combined_rerank(query, candidates, sim_scores, alpha=0.5):\n",
    "    sim_norm = [ (s + 1.0) / 2.0 for s in sim_scores ]\n",
    "    rer_scores = rerank_scores(query, candidates)\n",
    "    combined = []\n",
    "    for c, s_f, s_r in zip(candidates, sim_norm, rer_scores):\n",
    "        combined.append((c, (s_f * (1-alpha) + s_r * alpha)))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "    texts_sorted = [c for c, sc in combined_sorted]\n",
    "    scores_sorted = [sc for c, sc in combined_sorted]\n",
    "    return texts_sorted, scores_sorted, rer_scores\n",
    "\n",
    "def answer_query(query, retrieved_texts):\n",
    "    context = \"\\n\\n\".join(f\"[{i+1}] {t}\" for i, t in enumerate(retrieved_texts))\n",
    "    prompt = f\"Use the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    enc = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = llm_model.generate(**enc, max_length=256, num_beams=4, early_stopping=True)\n",
    "    return llm_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "example_query = \"I want to play the game with Motorcycles?\"\n",
    "retrieved, sim_scores, ids = retrieve(example_query, top_k=TOP_K)\n",
    "combined_texts, combined_scores, rer_scores = combined_rerank(example_query, retrieved, sim_scores, alpha=0.5)\n",
    "answer = answer_query(example_query, combined_texts[:TOP_K])\n",
    "print(\"Retrieved (FAISS):\")\n",
    "for t, s in zip(retrieved, sim_scores):\n",
    "    print(f\"score={s:.4f}\\t{t[:200]}\")\n",
    "print(\"\\nCombined (FAISS+Reranker):\")\n",
    "for t, s in zip(combined_texts[:TOP_K], combined_scores[:TOP_K]):\n",
    "    print(f\"score={s:.4f}\\t{t[:200]}\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and variant without blending scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "llm_model.eval()\n",
    "def retrieve(query, top_k=TOP_K):\n",
    "    q_enc = emb_tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    q_enc = {k: v.to(DEVICE) for k, v in q_enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**q_enc, output_hidden_states=False, return_dict=True)\n",
    "        if getattr(out, \"pooler_output\", None) is not None:\n",
    "            q_emb = out.pooler_output.cpu().numpy()\n",
    "        else:\n",
    "            q_emb = mean_pooling(out.last_hidden_state, q_enc[\"attention_mask\"]).cpu().numpy()\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    ids = I[0].tolist()\n",
    "    scores = D[0].tolist()\n",
    "    retrieved_texts = [texts[idx] for idx in ids]\n",
    "    return retrieved_texts, scores, ids\n",
    "\n",
    "def rerank(query, candidates, batch_size=RERANK_BATCH):\n",
    "    pairs = [f\"Query: {query}\\nCandidate: {c}\" for c in candidates]\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch_pairs = pairs[i:i+batch_size]\n",
    "            enc = reranker_tokenizer(batch_pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            out = reranker_model(**enc)\n",
    "            logits = out.logits\n",
    "            if logits.dim() == 1:\n",
    "                batch_scores = logits.cpu().numpy().tolist()\n",
    "            elif logits.dim() == 2 and logits.shape[1] == 1:\n",
    "                batch_scores = logits.squeeze(1).cpu().numpy().tolist()\n",
    "            else:\n",
    "                probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "                batch_scores = probs.tolist()\n",
    "            scores.extend(batch_scores)\n",
    "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "    reranked_texts = [r[0] for r in ranked]\n",
    "    reranked_scores = [r[1] for r in ranked]\n",
    "    return reranked_texts, reranked_scores\n",
    "\n",
    "def answer_query(query, retrieved_texts):\n",
    "    context = \"\\n\\n\".join(f\"[{i+1}] {t}\" for i, t in enumerate(retrieved_texts))\n",
    "    prompt = f\"Use the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    enc = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = llm_model.generate(**enc, max_length=256, num_beams=4, early_stopping=True)\n",
    "    return llm_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "example_query = \"Give me the name of the game for playing with cars, i need name of the game, not a number\"\n",
    "retrieved, sim_scores, ids = retrieve(example_query, top_k=TOP_K)\n",
    "reranked_texts, reranked_scores = rerank(example_query, retrieved)\n",
    "answer = answer_query(example_query, reranked_texts[:TOP_K])\n",
    "print(\"Retrieved (FAISS):\")\n",
    "for t, s in zip(retrieved, sim_scores):\n",
    "    print(f\"score={s:.4f}\\t{t[:200]}\")\n",
    "print(\"\\nReranked:\")\n",
    "for t, s in zip(reranked_texts[:TOP_K], reranked_scores[:TOP_K]):\n",
    "    print(f\"score={s:.4f}\\t{t[:200]}\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATA_CSV = \"sft_data.csv\"\n",
    "PROMPT_COL = \"prompt\"\n",
    "RESPONSE_COL = \"response\"\n",
    "OUTPUT_DIR = \"lora_finetuned\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LR = 3e-4\n",
    "MAX_LENGTH = 512\n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_ACCUM = 1\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\",\"c_proj\"]\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(DEVICE)\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, prompt_col=\"prompt\", response_col=\"response\", max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = dataframe[prompt_col].fillna(\"\").astype(str).tolist()\n",
    "        self.responses = dataframe[response_col].fillna(\"\").astype(str).tolist()\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.prompts[idx]\n",
    "        response = self.responses[idx]\n",
    "        prompt_ids = self.tokenizer(prompt, truncation=True, max_length=self.max_length, return_tensors=None)[\"input_ids\"]\n",
    "        full = prompt + response\n",
    "        enc = self.tokenizer(full, truncation=True, max_length=self.max_length, return_tensors=None)\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        labels = input_ids.copy()\n",
    "        p_len = len(prompt_ids)\n",
    "        for i in range(min(p_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "        return {\"input_ids\": torch.tensor(input_ids, dtype=torch.long), \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch]\n",
    "    max_len = max([t.size(0) for t in input_ids])\n",
    "    padded_inputs = torch.full((len(input_ids), max_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    padded_labels = torch.full((len(labels), max_len), -100, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((len(input_ids), max_len), dtype=torch.long)\n",
    "    for i, (inp, lab) in enumerate(zip(input_ids, labels)):\n",
    "        l = inp.size(0)\n",
    "        padded_inputs[i, :l] = inp\n",
    "        padded_labels[i, :l] = lab\n",
    "        attention_mask[i, :l] = 1\n",
    "    return {\"input_ids\": padded_inputs, \"attention_mask\": attention_mask, \"labels\": padded_labels}\n",
    "\n",
    "train_frac = 0.9\n",
    "n = len(df)\n",
    "idx = list(range(n))\n",
    "random.shuffle(idx)\n",
    "split = int(train_frac * n)\n",
    "train_idx = idx[:split]\n",
    "val_idx = idx[split:]\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "train_dataset = SFTDataset(train_df, tokenizer, PROMPT_COL, RESPONSE_COL, max_length=MAX_LENGTH)\n",
    "val_dataset = SFTDataset(val_df, tokenizer, PROMPT_COL, RESPONSE_COL, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = math.ceil(len(train_loader) * EPOCHS / GRAD_ACCUM)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.06 * total_steps), num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / GRAD_ACCUM\n",
    "        loss.backward()\n",
    "        if (step + 1) % GRAD_ACCUM == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    nb = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item() * input_ids.size(0)\n",
    "            nb += input_ids.size(0)\n",
    "    val_loss = total_loss / nb if nb > 0 else 0.0\n",
    "    model.train()\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-training\n",
    "\n",
    "https://github.com/unslothai/unsloth\n",
    "\n",
    "https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_DIR = \"/path/to/model_folder\"  # содержит config.json и pytorch_model.bin / *.safetensors и токенизатор\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\",              # распределит по GPU/CPU если доступно\n",
    "    torch_dtype=torch.float16       # если модель fp16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "texts = [\"Hello world\"]\n",
    "enc = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**enc, max_new_tokens=64, do_sample=False)\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
