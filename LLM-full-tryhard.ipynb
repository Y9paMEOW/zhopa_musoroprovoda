{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Допустим, у вас есть DataFrame с текстовым столбцом 'text'\n",
    "df = pd.DataFrame({'text': ['пример 1', 'пример 2', 'пример 3']})\n",
    "\n",
    "# Функция, которая применяет модель к одному элементу (промпту)\n",
    "def apply_model_to_prompt(text):\n",
    "    # Здесь код вызова модели по промпту с текстом\n",
    "    # Например, результат - просто текст с допиской для примера\n",
    "    return f\"Результат модели: {text}\"\n",
    "\n",
    "# Применяем функцию к столбцу 'text'\n",
    "df['model_result'] = df['text'].apply(apply_model_to_prompt)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e137495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код:\n",
    "\"Напиши оптимальное решение на Python для задачи: [описание]. Используй только чистый код без пояснений.\"\n",
    "\n",
    "# Математика:\n",
    "\"Реши задачу: [условие]. Действуй шаг за шагом. Ответ выведи в виде \\\\boxed{...}\"\n",
    "\n",
    "# Классификация:\n",
    "\"Текст: '{text}'. Выбери один класс: {labels}. Ответь только названием класса.\"\n",
    "\n",
    "# Изображение:\n",
    "\"Опиши содержимое изображения. Что на нём изображено? Ответь кратко.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "ОЛИМПИАДНЫЙ LLM-ФРЕЙМВОРК: ПРЯМОЙ КОД ДЛЯ КОПИРОВАНИЯ — БЕЗ АБСТРАКЦИЙ\n",
    "===================================================================================\n",
    "\n",
    "Цель: 1 задача = 1 копируемый кусок кода.  \n",
    "Модель = конкретная строка.  \n",
    "Промпт = конкретный формат.  \n",
    "Результат = готовый ответ.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# === СЛУЧАЙ 1: ГЕНЕРАЦИЯ КОДА — Llama-3-8B-Instruct ===\n",
    "# Модель: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# Задача: генерация оптимального решения на Python\n",
    "def solve_code_generation_llama3():\n",
    "    \"\"\"\n",
    "    Генерация кода для олимпиадных задач (Codeforces, AtCoder, LeetCode).\n",
    "    Модель: Llama-3-8B-Instruct (для локального запуска)\n",
    "    Промпт: системный + пользовательский (чат-формат)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    # Загрузка модели\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Промпт для задачи\n",
    "    problem_description = \"\"\"\n",
    "    Задача: Дан массив целых чисел. Найди максимальное произведение двух чисел.\n",
    "    Ограничения: 2 <= n <= 10^5, -10^3 <= arr[i] <= 10^3\n",
    "    Требуется: O(n log n) или O(n) решение на Python\n",
    "    \"\"\"\n",
    "\n",
    "    # Формирование промпта (чат-формат Llama-3)\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {problem_description}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    ```python\n",
    "    def max_product(arr):\n",
    "        arr.sort()\n",
    "        n = len(arr)\n",
    "        # Произведение двух наибольших чисел\n",
    "        max1 = arr[-1] * arr[-2]\n",
    "        # Произведение двух наименьших (если оба отрицательные)\n",
    "        max2 = arr[0] * arr[1]\n",
    "        return max(max1, max2)\n",
    "\n",
    "    # Пример использования\n",
    "    arr = [3, 4, 5, 2]\n",
    "    print(max_product(arr))  # 20\n",
    "    ```\n",
    "\n",
    "    \"\"\"  # <|eot_id|> — конец ответа для Llama-3\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.1,  # детерминированность\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # Извлечение только ответа (после assistant)\n",
    "    assistant_start = response.find(\"<|start_header_id|>assistant<|end_header_id|>\") + len(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "    code_response = response[assistant_start:].split(\"<|eot_id|>\")[0].strip()\n",
    "    return code_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 2: МАТЕМАТИЧЕСКОЕ РАССУЖДЕНИЕ — Mistral-7B-Instruct ===\n",
    "# Модель: mistralai/Mistral-7B-Instruct-v0.3\n",
    "# Задача: доказательство, решение уравнений, олимпиадная математика\n",
    "def solve_math_reasoning_mistral():\n",
    "    \"\"\"\n",
    "    Математические задачи с рассуждением.\n",
    "    Модель: Mistral-7B-Instruct (хорош для логики и шагов)\n",
    "    Промпт: [INST]...[/INST] — формат для Mistral\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    problem = \"\"\"\n",
    "    Решите уравнение: log₂(x) + log₂(x-3) = 2\n",
    "    Укажи допустимые значения x и приведи полное решение.\n",
    "    \"\"\"\n",
    "\n",
    "    # Формат для Mistral: [INST]...[/INST]\n",
    "    prompt = f\"\"\"[INST]{problem}[/INST]\n",
    "\n",
    "    Шаг 1: Найдем ОДЗ. log₂(x) определен при x > 0, log₂(x-3) при x-3 > 0 → x > 3.\n",
    "    Следовательно, x > 3.\n",
    "\n",
    "    Шаг 2: Применим свойство логарифмов: log_a(m) + log_a(n) = log_a(mn).\n",
    "    log₂(x) + log₂(x-3) = log₂(x(x-3)) = log₂(x² - 3x) = 2\n",
    "\n",
    "    Шаг 3: По определению логарифма: x² - 3x = 2² = 4\n",
    "    x² - 3x - 4 = 0\n",
    "\n",
    "    Шаг 4: Решим квадратное уравнение:\n",
    "    D = 9 + 16 = 25\n",
    "    x = (3 ± 5)/2 → x₁ = 4, x₂ = -1\n",
    "\n",
    "    Шаг 5: Проверка ОДЗ: x > 3. Только x = 4 подходит.\n",
    "\n",
    "    Ответ: x = 4\n",
    "    Проверка: log₂(4) + log₂(1) = 2 + 0 = 2 ✓\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Извлечение ответа (после [/INST])\n",
    "    if \"[/INST]\" in response:\n",
    "        return response.split(\"[/INST]\")[1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c12c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 3: МАССОВЫЙ ИНФЕРЕНС — vLLM (Mistral-7B) ===\n",
    "# Модель: mistralai/Mistral-7B-Instruct-v0.3\n",
    "# Задача: обработка 100+ запросов быстро\n",
    "def batch_inference_vllm():\n",
    "    \"\"\"\n",
    "    Высокопроизводительный инференс для олимпиад с vLLM.\n",
    "    Модель: Mistral-7B (быстрый, хорош для параллельного инференса)\n",
    "    Использование: когда нужно быстро обработать много задач.\n",
    "    \"\"\"\n",
    "    from vllm import LLM, SamplingParams\n",
    "\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        dtype=\"half\",\n",
    "        max_model_len=2048\n",
    "    )\n",
    "\n",
    "    # Запросы для обработки\n",
    "    prompts = [\n",
    "        \"[INST]Найди НОД(48, 18)[/INST]\",\n",
    "        \"[INST]Реши: 2x + 5 = 13[/INST]\",\n",
    "        \"[INST]Что такое бином Ньютона?[/INST]\",\n",
    "        \"[INST]Как работает алгоритм Дейкстры?[/INST]\"\n",
    "    ]\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    results = []\n",
    "    for output in outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        # Извлечение после [/INST]\n",
    "        if \"[/INST]\" in generated_text:\n",
    "            extracted = generated_text.split(\"[/INST]\")[1].strip()\n",
    "        else:\n",
    "            extracted = generated_text.strip()\n",
    "        results.append(extracted)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce54bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 4: МУЛЬТИМОДАЛЬНОЕ РАССУЖДЕНИЕ — Qwen-VL ===\n",
    "# Модель: Qwen/Qwen-VL-Chat\n",
    "# Задача: анализ изображений с текстом (графики, диаграммы, схемы)\n",
    "def solve_vision_qwen_vl():\n",
    "    \"\"\"\n",
    "    Визуально-языковое моделирование для задач с изображениями.\n",
    "    Модель: Qwen-VL (отлично для анализа графиков, диаграмм)\n",
    "    Вход: изображение + текстовый вопрос\n",
    "    \"\"\"\n",
    "    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "\n",
    "    model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "    processor = Qwen2VLProcessor.from_pretrained(model_name)\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "    # Загрузка изображения (заменить на свой путь)\n",
    "    # image_url = \"https://example.com/graph.png\"\n",
    "    # image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    image_path = \"graph.png\"  # Укажи путь к изображению\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    question = \"Какова максимальная точка на графике? Ответь числом.\"\n",
    "\n",
    "    # Формирование сообщения (чат-формат)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Применение шаблона чата\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Обработка изображений\n",
    "    image_inputs, video_inputs = processor.process_images([image], return_tensors=\"pt\")\n",
    "    inputs = processor(text=text, images=image_inputs, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "    \n",
    "    response = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    # Извлечение ответа после вопроса\n",
    "    if question in response:\n",
    "        return response.split(question)[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 5: КЛАССИФИКАЦИЯ ТЕКСТА — Llama-3-8B ===\n",
    "# Модель: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "# Задача: определение типа задачи (алгоритмы, математика, геометрия)\n",
    "def classify_task_llama3():\n",
    "    \"\"\"\n",
    "    Классификация типа задачи для олимпиад.\n",
    "    Модель: Llama-3 (хорош для точной классификации с инструкциями)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    text = \"Даны координаты трех точек. Определить, лежат ли они на одной прямой.\"\n",
    "    classes = [\"алгоритмы\", \"математика\", \"геометрия\", \"теория_чисел\", \"комбинаторика\"]\n",
    "\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Текст задачи: {text}\n",
    "Классы: {', '.join(classes)}\n",
    "Выбери наиболее подходящий класс. Ответь только названием класса.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "геометрия\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=32,  # только класс\n",
    "            temperature=0.0,    # детерминированность\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    assistant_start = response.find(\"<|start_header_id|>assistant<|end_header_id|>\") + len(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "    predicted_class = response[assistant_start:].split(\"<|eot_id|>\")[0].strip()\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 6: РАБОТА С API — GPT-4o ===\n",
    "# Модель: gpt-4o (через OpenAI API)\n",
    "# Задача: высокоточное решение сложных задач\n",
    "def solve_with_gpt4o():\n",
    "    \"\"\"\n",
    "    Использование GPT-4o через API для сложных задач.\n",
    "    Модель: gpt-4o (самая точная, но платная)\n",
    "    Требуется: export OPENAI_API_KEY=your_key\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    import os\n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    problem = \"\"\"\n",
    "    В треугольнике ABC известны стороны: AB = 13, BC = 14, AC = 15.\n",
    "    Найдите площадь треугольника и радиус вписанной окружности.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": problem}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === СЛУЧАЙ 7: МУЛЬТИМОДАЛЬНАЯ КЛАССИФИКАЦИЯ — BLIP-2 ===\n",
    "# Модель: Salesforce/blip2-flan-t5-xl\n",
    "# Задача: классификация изображений с текстовыми пояснениями\n",
    "def classify_image_blip2():\n",
    "    \"\"\"\n",
    "    Классификация изображений с текстовым описанием.\n",
    "    Модель: BLIP-2 (T5-xl) — гибрид Vision-Language\n",
    "    \"\"\"\n",
    "    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "    from PIL import Image\n",
    "\n",
    "    model_name = \"Salesforce/blip2-flan-t5-xl\"\n",
    "    processor = Blip2Processor.from_pretrained(model_name)\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    image_path = \"chart.png\"  # Укажи путь к изображению\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    prompt = \"Опиши, что изображено на графике. Это линейный или экспоненциальный рост?\"\n",
    "\n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    response = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ПРОМПТ ИНЖЕНЕРИНГ: ШАБЛОНЫ ДЛЯ РАЗНЫХ ТИПОВ ЗАДАЧ ===\n",
    "\"\"\"\n",
    "=== ШАБЛОНЫ ПРОМПТОВ ===\n",
    "\n",
    "1. КОДИРОВАНИЕ:\n",
    "```\n",
    "Задача: {description}\n",
    "Ограничения: {constraints}\n",
    "Язык: Python\n",
    "Требуется: {requirements}\n",
    "Формат: только код в блоке ```python...```\n",
    "```\n",
    "\n",
    "2. МАТЕМАТИКА:\n",
    "```\n",
    "Дано: {problem}\n",
    "Требуется: {solution_type}\n",
    "Формат: шаг за шагом, затем \\\\boxed{{ответ}}\n",
    "```\n",
    "\n",
    "3. КЛАССИФИКАЦИЯ:\n",
    "```\n",
    "Текст: {text}\n",
    "Классы: {classes}\n",
    "Ответ: только один класс\n",
    "```\n",
    "\n",
    "4. РАССУЖДЕНИЕ:\n",
    "```\n",
    "Ситуация: {context}\n",
    "Вопрос: {question}\n",
    "Формат: объяснение → вывод\n",
    "```\n",
    "\n",
    "5. МУЛЬТИМОДАЛЬНОСТЬ:\n",
    "```\n",
    "Изображение: {image}\n",
    "Текст: {text}\n",
    "Задание: {task}\n",
    "Ответ: {expected_format}\n",
    "```\n",
    "\n",
    "=== ВАРИАТИВНОСТЬ ПРОМПТОВ ===\n",
    "\n",
    "- **Zero-shot**: Просто задай задачу — \"Реши: x^2 - 5x + 6 = 0\"\n",
    "- **Few-shot**: Дай 1-2 примера — \"Пример: 2x=4 → x=2. Реши: 3x=9\"\n",
    "- **Chain-of-Thought**: \"Рассуждай шаг за шагом: 1) ..., 2) ..., 3) ...\"\n",
    "- **Instruction Tuning**: \"Ты — олимпиадный математик. Реши задачу строго по формулам.\"\n",
    "- **Role Prompting**: \"Ты — опытный программист на Python. Напиши эффективное решение.\"\n",
    "\n",
    "=== ПАРАМЕТРЫ ГЕНЕРАЦИИ ===\n",
    "\n",
    "- `temperature=0.0`: детерминированность (для тестов)\n",
    "- `temperature=0.7`: разнообразие (для творческих задач)\n",
    "- `top_p=0.9`: nucleus sampling (баланс качества и разнообразия)\n",
    "- `max_new_tokens=N`: ограничение длины ответа\n",
    "- `do_sample=True/False`: использовать ли семплирование\n",
    "- `repetition_penalty=1.2`: избегать повторений\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ea22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ПРИМЕРЫ ЗАПУСКА ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 1. ГЕНЕРАЦИЯ КОДА (Llama-3) ===\")\n",
    "    code_result = solve_code_generation_llama3()\n",
    "    print(code_result[:500] + \"...\" if len(code_result) > 500 else code_result)\n",
    "\n",
    "    print(\"\\n=== 2. МАТЕМАТИКА (Mistral) ===\")\n",
    "    math_result = solve_math_reasoning_mistral()\n",
    "    print(math_result[:300] + \"...\" if len(math_result) > 300 else math_result)\n",
    "\n",
    "    print(\"\\n=== 3. МАССОВЫЙ ИНФЕРЕНС (vLLM) ===\")\n",
    "    batch_results = batch_inference_vllm()\n",
    "    for i, res in enumerate(batch_results):\n",
    "        print(f\"Запрос {i+1}: {res[:100]}...\")\n",
    "\n",
    "    print(\"\\n=== 4. КЛАССИФИКАЦИЯ ТЕКСТА (Llama-3) ===\")\n",
    "    class_result = classify_task_llama3()\n",
    "    print(f\"Предсказанный класс: {class_result}\")\n",
    "\n",
    "    # Для GPT-4o раскомментировать если ключ установлен\n",
    "    # print(\"\\n=== 5. GPT-4o (API) ===\")\n",
    "    # gpt_result = solve_with_gpt4o()\n",
    "    # print(gpt_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
