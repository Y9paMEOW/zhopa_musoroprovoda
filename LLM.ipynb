{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83da9bb",
   "metadata": {},
   "source": [
    "text cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0efb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny2\"  # или ваша модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def predict(text):\n",
    "    return classifier(text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103a5d5",
   "metadata": {},
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb929e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "def predict_qa(context, question, model_name=\"deepset/roberta-base-squad2\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_scores, dim=1).item()\n",
    "    end_idx = torch.argmax(end_scores, dim=1).item()\n",
    "\n",
    "    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_idx:end_idx+1])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f1fd8",
   "metadata": {},
   "source": [
    "get embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14070ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # CLS или mean pooling\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e679d",
   "metadata": {},
   "source": [
    "txt similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def similarity(text1, text2):\n",
    "    emb1 = model.encode([text1])\n",
    "    emb2 = model.encode([text2])\n",
    "    return cosine_similarity(emb1, emb2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc7ad8",
   "metadata": {},
   "source": [
    "text2json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_json(text):\n",
    "    input_text = f\"parse: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs.input_ids, max_length=512, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1067ed9",
   "metadata": {},
   "source": [
    "zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2de844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def classify(text, labels):\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b43df",
   "metadata": {},
   "source": [
    "rag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60157afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, rag_model_name=\"facebook/rag-sequence-base\"):\n",
    "        self.tokenizer = RagTokenizer.from_pretrained(rag_model_name)\n",
    "        self.retriever = RagRetriever.from_pretrained(rag_model_name, index_name=\"exact\", use_dummy_dataset=True)\n",
    "        self.generator = RagSequenceForGeneration.from_pretrained(rag_model_name)\n",
    "\n",
    "    def generate(self, question):\n",
    "        inputs = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.generator.generate(**inputs)\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24573dd6",
   "metadata": {},
   "source": [
    "semantic txt similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_sts_score(text1, text2, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb1 = model.encode([text1])\n",
    "    emb2 = model.encode([text2])\n",
    "    score = cosine_similarity(emb1, emb2)[0][0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56786803",
   "metadata": {},
   "source": [
    "cls paraphrase detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bdfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def predict_paraphrase(text1, text2, model_name=\"sbert-base-uncased\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(text1, text2, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca80a3",
   "metadata": {},
   "source": [
    "zero shot cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29473e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def few_shot_classify(text, labels, model_name=\"gpt2\"):\n",
    "    # GPT-2 не имеет классификации, но можно использовать инференс через промпт\n",
    "    # Или использовать `text-generation` с кастомным промптом\n",
    "    classifier = pipeline(\"text-generation\", model=model_name, tokenizer=model_name)\n",
    "    prompt = f\"Classify the following text into one of these categories: {', '.join(labels)}.\\nText: {text}\\nCategory:\"\n",
    "    result = classifier(prompt, max_length=100, num_return_sequences=1)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14eedd5",
   "metadata": {},
   "source": [
    "qlora + peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def setup_lora_training(model_name, dataset):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./lora_results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_steps=100,\n",
    "        logging_steps=10,\n",
    "        report_to=None\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./lora_adapted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a66c54",
   "metadata": {},
   "source": [
    "fully shared data parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp import MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "\n",
    "def setup_fsdp(model):\n",
    "    auto_wrap_policy = transformer_auto_wrap_policy(\n",
    "        transformer_layer_cls={GPT2Block}\n",
    "    )\n",
    "    mp_policy = MixedPrecision(\n",
    "        param_dtype=torch.float16,\n",
    "        reduce_dtype=torch.float16,\n",
    "        buffer_dtype=torch.float16\n",
    "    )\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=auto_wrap_policy,\n",
    "        mixed_precision=mp_policy,\n",
    "        device_id=torch.cuda.current_device()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e81250",
   "metadata": {},
   "source": [
    "deepseak no, deepseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eccc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def setup_deepspeed(model, dataset, output_dir=\"./deepspeed_results\"):\n",
    "    ds_config = {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2,\n",
    "            \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "            \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True}\n",
    "        },\n",
    "        \"fp16\": {\"enabled\": True}\n",
    "    }\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        deepspeed=ds_config,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7d553",
   "metadata": {},
   "source": [
    "inf + cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from hashlib import md5\n",
    "\n",
    "def get_cached_embeddings(texts, model, cache_path=\"embeddings.pkl\"):\n",
    "    key = md5(str(texts).encode()).hexdigest()\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "\n",
    "    embeddings = get_embeddings(texts)  # функция из предыдущего примера\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "    else:\n",
    "        cache = {}\n",
    "\n",
    "    cache[key] = embeddings\n",
    "\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191006f2",
   "metadata": {},
   "source": [
    "faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    return index\n",
    "\n",
    "def search_faiss(query_embedding, index, k=5):\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    scores, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a9c17",
   "metadata": {},
   "source": [
    "rag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRReader\n",
    "import torch\n",
    "\n",
    "class DPRRAG:\n",
    "    def __init__(self, question_model, context_model, reader_model):\n",
    "        self.question_encoder = DPRQuestionEncoder.from_pretrained(question_model)\n",
    "        self.context_encoder = DPRContextEncoder.from_pretrained(context_model)\n",
    "        self.reader = DPRReader.from_pretrained(reader_model)\n",
    "        self.index = None\n",
    "        self.contexts = []\n",
    "\n",
    "    def encode_contexts(self, contexts):\n",
    "        self.contexts = contexts\n",
    "        embeddings = []\n",
    "        for ctx in contexts:\n",
    "            ctx_emb = self.context_encoder(ctx, return_tensors=\"pt\").pooler_output.detach().numpy()\n",
    "            embeddings.append(ctx_emb)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        self.index = build_faiss_index(embeddings)\n",
    "\n",
    "    def retrieve(self, query, k=5):\n",
    "        query_emb = self.question_encoder(query, return_tensors=\"pt\").pooler_output.detach().numpy()\n",
    "        scores, indices = search_faiss(query_emb, self.index, k=k)\n",
    "        return [self.contexts[i] for i in indices[0]]\n",
    "\n",
    "    def read(self, question, context):\n",
    "        inputs = self.reader(question=question, titles=[\"\"], texts=[context], return_tensors=\"pt\")\n",
    "        outputs = self.reader(**inputs)\n",
    "        start_idx = outputs.start_logits.argmax().item()\n",
    "        end_idx = outputs.end_logits.argmax().item()\n",
    "        answer = self.reader.convert_tokens_to_string(\n",
    "            inputs.input_ids[0][start_idx:end_idx+1]\n",
    "        )\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad8cb4",
   "metadata": {},
   "source": [
    "subform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59eb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_submission(predictions, output_path=\"submission.json\"):\n",
    "    \"\"\"\n",
    "    predictions: list of answers in order of questions\n",
    "    output_path: path to save JSON submission\n",
    "    \"\"\"\n",
    "    submission = {\"answers\": predictions}\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Submission saved to {output_path}\")\n",
    "\n",
    "def load_sample_submission(path=\"sample_submission.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sample = json.load(f)\n",
    "    return sample\n",
    "\n",
    "def match_submission_to_sample(sample_path, predictions):\n",
    "    sample = load_sample_submission(sample_path)\n",
    "    sample[\"answers\"] = predictions\n",
    "    return sample\n",
    "\n",
    "def save_submission_with_sample(sample_path, predictions, output_path=\"submission.json\"):\n",
    "    submission = match_submission_to_sample(sample_path, predictions)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Submission saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(queries, contexts, output_path=\"submission.json\"):\n",
    "    rag = DPRRAG(\n",
    "        question_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "        context_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "        reader_model=\"facebook/dpr-reader-single-nq-base\"\n",
    "    )\n",
    "    rag.encode_contexts(contexts)\n",
    "\n",
    "    answers = []\n",
    "    for query in queries:\n",
    "        retrieved = rag.retrieve(query, k=1)[0]\n",
    "        answer = rag.read(query, retrieved)\n",
    "        answers.append(answer)\n",
    "\n",
    "    save_submission_with_sample(\"sample_submission.json\", answers, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
