{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbfa454",
   "metadata": {},
   "source": [
    "```yaml\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .py\n",
    "      format_name: light\n",
    "      format_version: '1.5'\n",
    "      jupytext_version: 1.16.4\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    name: python3\n",
    "```\n",
    "Ниже приведены основные компоненты:\n",
    "- Загрузка и предобработка аудио\n",
    "- Модели: CNN, Transformer, Whisper\n",
    "- Аугментации\n",
    "- Метрики\n",
    "- Тренировка и инференс\n",
    "\n",
    "**Как использовать:**\n",
    "- Замените пути к данным на свои\n",
    "- Подставьте свои метки и классы\n",
    "- Выберите нужную задачу и модель\n",
    "- Настройте гиперпараметры под себя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchaudio librosa audiomentations transformers datasets scikit-learn wandb tqdm pandas numpy\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- НАСТРОЙКИ ---\n",
    "# Подставьте сюда путь к вашему датасету\n",
    "DATA_PATH = \"/path/to/your/audio/data\"\n",
    "MODEL_NAME = \"openai/whisper-base\"  # Модель Whisper (base, tiny, small, large)\n",
    "TASK_TYPE = \"vad\"  # \"vad\", \"classification\", \"sed\", \"transcription\"\n",
    "NUM_CLASSES = 2  # Для задачи \"я тебя не слышу\": 0 - нет речи, 1 - есть речь\n",
    "SAMPLE_RATE = 16000  # Whisper работает на 16kHz\n",
    "DURATION = 5.0  # в секундах\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60992228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr=16000, duration=5.0):\n",
    "    \"\"\"\n",
    "    Загрузка и нормализация аудио файла.\n",
    "    Если длина файла < duration, то дополняем нулями.\n",
    "    Если > duration, то обрезаем.\n",
    "    \"\"\"\n",
    "    waveform, orig_sr = torchaudio.load(path)\n",
    "    # Ресемплинг\n",
    "    if orig_sr != sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, sr)\n",
    "        waveform = resampler(waveform)\n",
    "    # Моно\n",
    "    if waveform.shape[0] == 2:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    # Обрезка/заполнение\n",
    "    expected_length = int(sr * duration)\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < expected_length:\n",
    "        # Заполняем нулями\n",
    "        pad_length = expected_length - current_length\n",
    "        waveform = F.pad(waveform, (0, pad_length))\n",
    "    elif current_length > expected_length:\n",
    "        # Обрезаем\n",
    "        waveform = waveform[:, :expected_length]\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для работы с аудио-датасетом.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, sr=16000, duration=5.0, transform=None, task_type=\"vad\"):\n",
    "        self.df = df\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.transform = transform\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        waveform = load_audio(path, sr=self.sr, duration=self.duration)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform.numpy())\n",
    "            waveform = torch.tensor(waveform)\n",
    "\n",
    "        # Whisper принимает raw audio\n",
    "        if self.task_type == \"transcription\":\n",
    "            # Возвращаем waveform и текст (если есть)\n",
    "            text = self.df.iloc[idx].get('text', \"\")\n",
    "            return waveform, text\n",
    "        else:\n",
    "            # Для задач классификации и VAD\n",
    "            return waveform, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf52245",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.9, max_rate=1.1, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_fraction=-0.1, max_fraction=0.1, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN-базовая модель для VAD / Classification ---\n",
    "class SimpleAudioCNN(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=80, stride=4, padding=38)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.fc = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = torch.mean(x, dim=2)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1083b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка Whisper модели для транскрибции\n",
    "# Используется только если task_type == \"transcription\"\n",
    "if TASK_TYPE == \"transcription\":\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "    whisper_model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    whisper_model.to(DEVICE)\n",
    "    whisper_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример DataFrame (замените на свой)\n",
    "# df = pd.read_csv(\"your_data.csv\")  # с колонками: path, label, text (если transcription)\n",
    "# Пример:\n",
    "df = pd.DataFrame({\n",
    "    'path': ['/path/to/audio1.wav', '/path/to/audio2.wav'],\n",
    "    'label': [1, 0],  # 1 - речь, 0 - нет\n",
    "    'text': ['hello world', '']  # только для транскрибции\n",
    "})\n",
    "\n",
    "# Разделение\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(\n",
    "    train_df,\n",
    "    sr=SAMPLE_RATE,\n",
    "    duration=DURATION,\n",
    "    transform=augmentations,\n",
    "    task_type=TASK_TYPE\n",
    ")\n",
    "\n",
    "val_dataset = AudioDataset(\n",
    "    val_df,\n",
    "    sr=SAMPLE_RATE,\n",
    "    duration=DURATION,\n",
    "    transform=None,\n",
    "    task_type=TASK_TYPE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d570b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TYPE in [\"vad\", \"classification\", \"sed\"]:\n",
    "    model = SimpleAudioCNN(n_classes=NUM_CLASSES)\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37694e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfedf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return total_loss / len(dataloader), acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84636ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TYPE in [\"vad\", \"classification\", \"sed\"]:\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, DEVICE)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_with_whisper(audio_path, processor, model, device):\n",
    "    \"\"\"\n",
    "    Транскрибация одного аудиофайла с помощью Whisper.\n",
    "    \"\"\"\n",
    "    waveform = load_audio(audio_path, sr=16000, duration=None)  # Whisper сам обрабатывает длину\n",
    "    input_features = processor(\n",
    "        waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    input_features = input_features.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0aac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TYPE == \"transcription\":\n",
    "    # Пример: транскрибация\n",
    "    sample_path = \"/path/to/sample.wav\"\n",
    "    result = transcribe_audio_with_whisper(sample_path, processor, whisper_model, DEVICE)\n",
    "    print(\"Transcription:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb75c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK_TYPE in [\"vad\", \"classification\", \"sed\"]:\n",
    "    torch.save(model.state_dict(), f\"model_{TASK_TYPE}.pth\")\n",
    "    print(f\"Модель {TASK_TYPE} сохранена как model_{TASK_TYPE}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230f511",
   "metadata": {},
   "source": [
    "## Как подпиливать под задачу:\n",
    "\n",
    "1. **Замените `df`** на ваш датафрейм с `path`, `label` (и `text` для транскрибации).\n",
    "2. **Измените `TASK_TYPE`** на `\"vad\"`, `\"classification\"`, `\"sed\"`, `\"transcription\"`.\n",
    "3. **Подставьте `NUM_CLASSES`** для вашей задачи.\n",
    "4. **Настройте `augmentations`**, если нужно.\n",
    "5. **Замените `SimpleAudioCNN`** на другую архитектуру (ResNet, PANN, Transformer).\n",
    "6. **Для Whisper** — используйте `transcribe_audio_with_whisper` для транскрибации или fine-tune.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53670596",
   "metadata": {},
   "source": [
    "1. VAD — Voice Activity Detection (детекция активности речи)\n",
    "2. Classification — классификация аудио\n",
    "3. SED — Sound Event Detection (детекция звуковых событий во времени)\n",
    "4. Transcription — транскрибация (ASR: Automatic Speech Recognition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
