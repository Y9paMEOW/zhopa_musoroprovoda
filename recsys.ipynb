{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== ДАТАСЕТЫ ====================\n",
    "\n",
    "class UniversalRecDataset(Dataset):\n",
    "    \"\"\"Универсальный датасет для всех типов задач рекомендаций\"\"\"\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 user_col: str = 'user_id',\n",
    "                 item_col: str = 'item_id', \n",
    "                 rating_col: Optional[str] = 'rating',\n",
    "                 time_col: Optional[str] = 'timestamp',\n",
    "                 sequence_length: int = 50,\n",
    "                 task_type: str = 'sequential'):  # 'classic', 'sequential', 'implicit'\n",
    "        \"\"\"\n",
    "        task_type:\n",
    "        - 'classic': с рейтингами (MSE loss)\n",
    "        - 'sequential': next-item prediction (NLL loss) \n",
    "        - 'implicit': implicit feedback (BPR loss)\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.rating_col = rating_col\n",
    "        self.time_col = time_col\n",
    "        self.sequence_length = sequence_length\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        if task_type == 'sequential':\n",
    "            # Подготовка для последовательных задач\n",
    "            self.df = self.df.sort_values([user_col, time_col]).reset_index(drop=True)\n",
    "            self.user_sequences = self.df.groupby(user_col)[item_col].apply(list).to_dict()\n",
    "            self.users = list(self.user_sequences.keys())\n",
    "        else:\n",
    "            # Для других задач просто сохраняем все взаимодействия\n",
    "            pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.task_type == 'sequential':\n",
    "            return len(self.users)\n",
    "        else:\n",
    "            return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.task_type == 'sequential':\n",
    "            user_id = self.users[idx]\n",
    "            items = self.user_sequences[user_id]\n",
    "            \n",
    "            # Берем последние N взаимодействий\n",
    "            if len(items) > self.sequence_length:\n",
    "                sequence = items[-self.sequence_length:]\n",
    "            else:\n",
    "                sequence = [items[0]] * (self.sequence_length - len(items)) + items\n",
    "            \n",
    "            return {\n",
    "                'user_id': int(user_id),\n",
    "                'sequence': torch.LongTensor(sequence[:-1]),\n",
    "                'target_item': torch.LongTensor([sequence[-1]])\n",
    "            }\n",
    "        else:\n",
    "            row = self.df.iloc[idx]\n",
    "            result = {\n",
    "                'user_id': int(row[self.user_col]),\n",
    "                'item_id': int(row[self.item_col])\n",
    "            }\n",
    "            \n",
    "            if self.rating_col and self.rating_col in row:\n",
    "                result['rating'] = float(row[self.rating_col])\n",
    "            \n",
    "            if self.time_col and self.time_col in row:\n",
    "                result['timestamp'] = float(row[self.time_col])\n",
    "                \n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== МОДЕЛИ ====================\n",
    "\n",
    "class UniversalRecModel(nn.Module):\n",
    "    \"\"\"Универсальная рекомендательная модель\"\"\"\n",
    "    def __init__(self, \n",
    "                 n_users: int, \n",
    "                 n_items: int,\n",
    "                 model_type: str = 'mf',  # 'mf', 'gru', 'transformer', 'two_tower'\n",
    "                 embedding_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 sequence_length: int = 50,\n",
    "                 n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Базовые эмбеддинги\n",
    "        self.user_embeddings = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        if model_type == 'mf':\n",
    "            # Matrix Factorization с bias\n",
    "            self.user_bias = nn.Embedding(n_users, 1)\n",
    "            self.item_bias = nn.Embedding(n_items, 1)\n",
    "            self.global_bias = nn.Parameter(torch.tensor(0.0))\n",
    "            \n",
    "        elif model_type == 'gru':\n",
    "            # GRU для последовательностей\n",
    "            self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "            self.output_projection = nn.Linear(hidden_dim, embedding_dim)\n",
    "            \n",
    "        elif model_type == 'transformer':\n",
    "            # Transformer для последовательностей\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim, \n",
    "                nhead=8, \n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "            self.output_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "            \n",
    "        elif model_type == 'two_tower':\n",
    "            # Two-tower архитектура\n",
    "            self.user_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "            self.item_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        if self.model_type in ['gru', 'transformer']:\n",
    "            # Для последовательных моделей\n",
    "            sequences = batch['sequence']  # (batch_size, seq_len)\n",
    "            embedded_seq = self.item_embeddings(sequences)  # (batch_size, seq_len, embed_dim)\n",
    "            \n",
    "            if self.model_type == 'gru':\n",
    "                gru_out, hidden = self.gru(embedded_seq)\n",
    "                # Используем последнее скрытое состояние\n",
    "                user_repr = self.output_projection(hidden[-1])  # (batch_size, embed_dim)\n",
    "                \n",
    "            elif self.model_type == 'transformer':\n",
    "                transformer_out = self.transformer(embedded_seq)\n",
    "                # Используем последнее состояние\n",
    "                user_repr = self.output_projection(transformer_out[:, -1, :])  # (batch_size, embed_dim)\n",
    "            \n",
    "            # Получаем эмбеддинги целевых айтемов\n",
    "            target_items = batch['target_item'].squeeze(-1)  # (batch_size,)\n",
    "            item_repr = self.item_embeddings(target_items)  # (batch_size, embed_dim)\n",
    "            \n",
    "            # Скалярное произведение\n",
    "            scores = torch.sum(user_repr * item_repr, dim=1)\n",
    "            return scores\n",
    "            \n",
    "        elif self.model_type == 'two_tower':\n",
    "            # Two-tower: отдельные эмбеддинги для юзера и айтема\n",
    "            user_ids = batch['user_id']\n",
    "            item_ids = batch['item_id']\n",
    "            \n",
    "            user_emb = F.normalize(self.user_embeddings(user_ids), p=2, dim=1)\n",
    "            item_emb = F.normalize(self.item_embeddings(item_ids), p=2, dim=1)\n",
    "            \n",
    "            return torch.sum(user_emb * item_emb, dim=1)\n",
    "            \n",
    "        else:  # Matrix Factorization\n",
    "            user_ids = batch['user_id']\n",
    "            item_ids = batch['item_id']\n",
    "            \n",
    "            user_emb = self.user_embeddings(user_ids)\n",
    "            item_emb = self.item_embeddings(item_ids)\n",
    "            \n",
    "            dot_product = torch.sum(user_emb * item_emb, dim=1)\n",
    "            \n",
    "            if hasattr(self, 'user_bias'):\n",
    "                user_bias = self.user_bias(user_ids).squeeze()\n",
    "                item_bias = self.item_bias(item_ids).squeeze()\n",
    "                return dot_product + user_bias + item_bias + self.global_bias\n",
    "            else:\n",
    "                return dot_product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1947005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== МЕТРИКИ ====================\n",
    "\n",
    "class RecMetrics:\n",
    "    \"\"\"Класс для вычисления рекомендательных метрик\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Precision@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_set = set(y_pred)\n",
    "        return len(y_true_set & y_pred_set) / len(y_pred_set) if y_pred_set else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Recall@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        y_true_set = set(y_true)\n",
    "        y_pred_set = set(y_pred)\n",
    "        return len(y_true_set & y_pred_set) / len(y_true_set) if y_true_set else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def map_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Mean Average Precision@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        \n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "        \n",
    "        for i, p in enumerate(y_pred):\n",
    "            if p in y_true and p not in y_pred[:i]:\n",
    "                num_hits += 1.0\n",
    "                score += num_hits / (i + 1.0)\n",
    "        \n",
    "        if not y_true:\n",
    "            return 0.0\n",
    "        \n",
    "        return score / min(len(y_true), k)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg_at_k(y_true: List[int], y_pred: List[int], k: int = 20) -> float:\n",
    "        \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "        if len(y_pred) > k:\n",
    "            y_pred = y_pred[:k]\n",
    "        \n",
    "        # Бинарная релевантность\n",
    "        relevance = [1.0 if item in y_true else 0.0 for item in y_pred]\n",
    "        \n",
    "        # DCG\n",
    "        dcg = sum(rel / np.log2(pos + 2) for pos, rel in enumerate(relevance))\n",
    "        \n",
    "        # IDCG\n",
    "        idcg = sum(1.0 / np.log2(pos + 2) for pos in range(min(len(y_true), k)))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0.0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== LIGHTNING МОДУЛЬ ====================\n",
    "\n",
    "class UniversalRecLightning(pl.LightningModule):\n",
    "    \"\"\"Универсальный Lightning модуль для всех задач рекомендаций\"\"\"\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 task_type: str = 'sequential',  # 'classic', 'sequential', 'implicit'\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 metrics: List[str] = ['map']):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.task_type = task_type\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.metrics = metrics\n",
    "        self.metrics_calculator = RecMetrics()\n",
    "        \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        return self.model(batch)\n",
    "    \n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        if self.task_type == 'classic':\n",
    "            # MSE для задач с рейтингами\n",
    "            predictions = self.model(batch)\n",
    "            ratings = batch['rating']\n",
    "            loss = F.mse_loss(predictions, ratings)\n",
    "            \n",
    "        elif self.task_type == 'sequential':\n",
    "            # Cross-entropy для next-item prediction\n",
    "            logits = self.model(batch)\n",
    "            target_items = batch['target_item'].squeeze(1)\n",
    "            loss = F.cross_entropy(logits, target_items)\n",
    "            \n",
    "        elif self.task_type == 'implicit':\n",
    "            # BPR loss для implicit feedback\n",
    "            scores = self.model(batch)\n",
    "            loss = -F.logsigmoid(scores).mean()\n",
    "            \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
    "        # Вычисление валидационных метрик\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(batch)\n",
    "            \n",
    "            if self.task_type == 'classic':\n",
    "                ratings = batch['rating']\n",
    "                val_loss = F.mse_loss(predictions, ratings)\n",
    "                self.log('val_loss', val_loss)\n",
    "                \n",
    "            elif self.task_type == 'sequential':\n",
    "                # Для последовательных задач можно вычислить accuracy\n",
    "                target_items = batch['target_item'].squeeze(1)\n",
    "                accuracy = (predictions.argmax(dim=1) == target_items).float().mean()\n",
    "                self.log('val_acc', accuracy)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min' if self.task_type == 'classic' else 'max',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss' if self.task_type == 'classic' else 'val_acc'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== ТРЕЙНЕР ====================\n",
    "\n",
    "class UniversalRecTrainer:\n",
    "    \"\"\"Универсальный трейнер для всех задач рекомендаций\"\"\"\n",
    "    def __init__(self,\n",
    "                 model_type: str = 'mf',  # 'mf', 'gru', 'transformer', 'two_tower'\n",
    "                 task_type: str = 'sequential',  # 'classic', 'sequential', 'implicit'\n",
    "                 embedding_dim: int = 128,\n",
    "                 hidden_dim: int = 256,\n",
    "                 sequence_length: int = 50,\n",
    "                 n_layers: int = 2,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 batch_size: int = 256,\n",
    "                 max_epochs: int = 50):\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.task_type = task_type\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        \n",
    "        self.user_encoder = None\n",
    "        self.item_encoder = None\n",
    "        self.model = None\n",
    "        self.lightning_model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "    def prepare_data(self, df: pd.DataFrame, \n",
    "                    user_col: str = 'user_id',\n",
    "                    item_col: str = 'item_id',\n",
    "                    rating_col: Optional[str] = 'rating',\n",
    "                    time_col: Optional[str] = 'timestamp',\n",
    "                    test_size: float = 0.2,\n",
    "                    val_size: float = 0.1):\n",
    "        \"\"\"Подготовка данных с энкодингом и разделением\"\"\"\n",
    "        \n",
    "        # Создаем энкодеры\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        \n",
    "        df[user_col] = self.user_encoder.fit_transform(df[user_col])\n",
    "        df[item_col] = self.item_encoder.fit_transform(df[item_col])\n",
    "        \n",
    "        # Разделение данных\n",
    "        if self.task_type == 'sequential':\n",
    "            # Для последовательных задач разбиваем по времени\n",
    "            df = df.sort_values([user_col, time_col])\n",
    "            # Берем последние interactions как тест\n",
    "            user_groups = df.groupby(user_col)\n",
    "            train_data, test_data = [], []\n",
    "            \n",
    "            for user_id, user_df in user_groups:\n",
    "                n_interactions = len(user_df)\n",
    "                split_idx = int(n_interactions * (1 - test_size))\n",
    "                train_data.append(user_df.iloc[:split_idx])\n",
    "                test_data.append(user_df.iloc[split_idx:])\n",
    "            \n",
    "            train_df = pd.concat(train_data)\n",
    "            test_df = pd.concat(test_data)\n",
    "            \n",
    "        else:\n",
    "            # Для других задач случайное разделение\n",
    "            train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Разделение трейна на трейн и валидацию\n",
    "        if len(train_df) > len(test_df):  # Убедимся что есть данные для валидации\n",
    "            train_df, val_df = train_test_split(train_df, test_size=val_size/(1-test_size), random_state=42)\n",
    "        else:\n",
    "            val_df = test_df  # Если данных мало, используем тест как валидацию\n",
    "        \n",
    "        # Создание датасетов\n",
    "        train_dataset = UniversalRecDataset(\n",
    "            train_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type\n",
    "        )\n",
    "        \n",
    "        val_dataset = UniversalRecDataset(\n",
    "            val_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type\n",
    "        )\n",
    "        \n",
    "        test_dataset = UniversalRecDataset(\n",
    "            test_df, user_col, item_col, rating_col, time_col, \n",
    "            self.sequence_length, self.task_type\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    def build_model(self, n_users: int, n_items: int):\n",
    "        \"\"\"Создание модели\"\"\"\n",
    "        self.model = UniversalRecModel(\n",
    "            n_users=n_users,\n",
    "            n_items=n_items,\n",
    "            model_type=self.model_type,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            sequence_length=self.sequence_length,\n",
    "            n_layers=self.n_layers\n",
    "        )\n",
    "        \n",
    "        self.lightning_model = UniversalRecLightning(\n",
    "            model=self.model,\n",
    "            task_type=self.task_type,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset, num_workers: int = 4):\n",
    "        \"\"\"Обучение модели\"\"\"\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='val_loss' if self.task_type == 'classic' else 'val_acc',\n",
    "            mode='min' if self.task_type == 'classic' else 'max',\n",
    "            save_top_k=1,\n",
    "            filename='best-{epoch:02d}-{val_loss:.2f}' if self.task_type == 'classic' \n",
    "                    else 'best-{epoch:02d}-{val_acc:.2f}'\n",
    "        )\n",
    "        \n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='val_loss' if self.task_type == 'classic' else 'val_acc',\n",
    "            patience=10,\n",
    "            mode='min' if self.task_type == 'classic' else 'max'\n",
    "        )\n",
    "        \n",
    "        # Создание трейнера\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=self.max_epochs,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            accelerator='auto',\n",
    "            devices='auto',\n",
    "            precision=16 if torch.cuda.is_available() else 32,\n",
    "            gradient_clip_val=1.0\n",
    "        )\n",
    "        \n",
    "        # Обучение\n",
    "        self.trainer.fit(\n",
    "            self.lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Обучение завершено. Лучшая модель: {checkpoint_callback.best_model_path}\")\n",
    "    \n",
    "    def predict_top_k(self, user_ids: List[int], k: int = 20) -> np.ndarray:\n",
    "        \"\"\"Генерация top-k рекомендаций для пользователей\"\"\"\n",
    "        self.lightning_model.eval()\n",
    "        \n",
    "        # Получаем все возможные айтемы\n",
    "        all_items = torch.arange(len(self.item_encoder.classes_))\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for user_id in user_ids:\n",
    "                user_tensor = torch.LongTensor([user_id])\n",
    "                \n",
    "                if self.model_type in ['gru', 'transformer']:\n",
    "                    # Для последовательных моделей нужно сгенерировать последовательность\n",
    "                    # Здесь упрощенный вариант - используем последние взаимодействия\n",
    "                    # В реальности нужно реализовать proper sequence generation\n",
    "                    scores = torch.zeros(len(self.item_encoder.classes_))\n",
    "                else:\n",
    "                    # Для других моделей\n",
    "                    user_items_scores = []\n",
    "                    for item_id in range(len(self.item_encoder.classes_)):\n",
    "                        batch = {\n",
    "                            'user_id': user_tensor,\n",
    "                            'item_id': torch.LongTensor([item_id])\n",
    "                        }\n",
    "                        score = self.lightning_model.model(batch)\n",
    "                        user_items_scores.append(score.item())\n",
    "                    \n",
    "                    scores = torch.tensor(user_items_scores)\n",
    "                \n",
    "                # Получаем top-k\n",
    "                top_k = torch.topk(scores, min(k, len(scores)))\n",
    "                predictions.append(top_k.indices.cpu().numpy())\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def generate_submission(self, test_users: List[int], k: int = 20, \n",
    "                           output_file: str = 'submission.csv'):\n",
    "        \"\"\"Генерация submission файла\"\"\"\n",
    "        predictions = self.predict_top_k(test_users, k)\n",
    "        \n",
    "        # Декодируем ID обратно\n",
    "        decoded_predictions = []\n",
    "        for pred in predictions:\n",
    "            decoded = self.item_encoder.inverse_transform(pred)\n",
    "            decoded_predictions.append(' '.join(map(str, decoded)))\n",
    "        \n",
    "        submission_df = pd.DataFrame({\n",
    "            'user_id': self.user_encoder.inverse_transform(test_users),\n",
    "            'item_ids': decoded_predictions\n",
    "        })\n",
    "        \n",
    "        submission_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Submission сохранен в {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1293b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== ПРИМЕРЫ ИСПОЛЬЗОВАНИЯ ====================\n",
    "\n",
    "def example_t_shopping():\n",
    "    \"\"\"Пример для T-Shopping задачи (next-item prediction)\"\"\"\n",
    "    # Загрузка данных\n",
    "    # df = pd.read_parquet('train_data.pq')  # user_id, item_id, date\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='gru',  # или 'transformer' для лучшего понимания последовательностей\n",
    "        task_type='sequential',\n",
    "        embedding_dim=256,  # увеличьте для сложных паттернов\n",
    "        hidden_dim=512,     # увеличьте для сложных зависимостей\n",
    "        sequence_length=100, # увеличьте если есть длинные сессии\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=128,\n",
    "        max_epochs=30\n",
    "    )\n",
    "    \n",
    "    # Подготовка данных\n",
    "    # train_dataset, val_dataset, test_dataset = trainer.prepare_data(\n",
    "    #     df, user_col='user_id', item_col='item_id', time_col='date'\n",
    "    # )\n",
    "    \n",
    "    # Создание модели\n",
    "    # trainer.build_model(\n",
    "    #     n_users=len(trainer.user_encoder.classes_),\n",
    "    #     n_items=len(trainer.item_encoder.classes_)\n",
    "    # )\n",
    "    \n",
    "    # Обучение\n",
    "    # trainer.train(train_dataset, val_dataset)\n",
    "    \n",
    "    # Генерация предсказаний для submission\n",
    "    # test_users = list(set(df['user_id']))  # или из sample_submission\n",
    "    # trainer.generate_submission(test_users, k=20, output_file='t_shopping_submission.csv')\n",
    "\n",
    "def example_classic_rating():\n",
    "    \"\"\"Пример для задач с явными рейтингами\"\"\"\n",
    "    # df = pd.read_csv('ratings.csv')  # user_id, item_id, rating\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='mf',  # matrix factorization для рейтингов\n",
    "        task_type='classic',\n",
    "        embedding_dim=128,\n",
    "        learning_rate=1e-3,\n",
    "        max_epochs=50\n",
    "    )\n",
    "    \n",
    "    # Аналогично: prepare_data -> build_model -> train\n",
    "\n",
    "def example_implicit_feedback():\n",
    "    \"\"\"Пример для implicit feedback задач\"\"\"\n",
    "    # df = pd.read_csv('clicks.csv')  # user_id, item_id (без рейтингов)\n",
    "    \n",
    "    trainer = UniversalRecTrainer(\n",
    "        model_type='two_tower',  # good for large-scale implicit\n",
    "        task_type='implicit',\n",
    "        embedding_dim=128,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "    \n",
    "    # Аналогично: prepare_data -> build_model -> train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a59987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== КОНФИГУРАЦИЯ ====================\n",
    "\"\"\"\n",
    "НАСТРОЙКИ ПОД КОНКРЕТНЫЕ ЗАДАЧИ:\n",
    "\n",
    "1. T-SHOPPING (Next Item Prediction):\n",
    "   - model_type: 'gru' или 'transformer'\n",
    "   - task_type: 'sequential' \n",
    "   - sequence_length: 50-200 (в зависимости от сессий)\n",
    "   - embedding_dim: 256-512\n",
    "   - Добавить temporal features в preprocessing\n",
    "\n",
    "2. MOVIELENS (Rating Prediction):\n",
    "   - model_type: 'mf'\n",
    "   - task_type: 'classic'\n",
    "   - loss: MSE\n",
    "   - embedding_dim: 64-128\n",
    "\n",
    "3. AMAZON (Implicit Feedback):\n",
    "   - model_type: 'two_tower'\n",
    "   - task_type: 'implicit'\n",
    "   - negative sampling: 10-20 negatives per positive\n",
    "\n",
    "4. YOUTUBE (Large-scale Retrieval):\n",
    "   - model_type: 'two_tower' \n",
    "   - batch_size: 1024+\n",
    "   - embedding_dim: 256+\n",
    "   - use popularity sampling for negatives\n",
    "\n",
    "ПАРАМЕТРЫ ДЛЯ ТЮНИНГА:\n",
    "- embedding_dim: 64, 128, 256, 512\n",
    "- hidden_dim: 128, 256, 512 (для sequential)\n",
    "- sequence_length: 20, 50, 100 (для sequential)\n",
    "- learning_rate: 1e-4, 1e-3, 1e-2\n",
    "- batch_size: 64, 128, 256, 512\n",
    "- max_epochs: 20-100 (с early stopping)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
