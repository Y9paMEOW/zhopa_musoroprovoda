{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "images_dir = \"path/to/images\"\n",
    "csv_file = \"path/to/annotations.csv\"\n",
    "output_dir = \"path/to/yolo_format\"\n",
    "\n",
    "annotations = pd.read_csv(csv_file)\n",
    "\n",
    "def normalize_coordinates(x1, y1, x2, y2, img_width, img_height):\n",
    "    x_center = (x1 + x2) / 2 / img_width\n",
    "    y_center = (y1 + y2) / 2 / img_height\n",
    "    width = (x2 - x1) / img_width\n",
    "    height = (y2 - y1) / img_height\n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "for img_name in annotations['filename'].unique():\n",
    "    img_path = os.path.join(images_dir, img_name)\n",
    "    img = Image.open(img_path)\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "    txt_file = os.path.join(output_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n",
    "\n",
    "    with open(txt_file, 'w') as f:\n",
    "        img_annotations = annotations[annotations['filename'] == img_name]\n",
    "\n",
    "        for _, row in img_annotations.iterrows():\n",
    "            class_id = row['class']\n",
    "            x_center, y_center, width, height = normalize_coordinates(\n",
    "                row['x1'], row['y1'], row['x2'], row['y2'], img_width, img_height\n",
    "            )\n",
    "            f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.yaml\n",
    "# train: path/to/dataset/images/train \n",
    "# val: path/to/dataset/images/val    \n",
    "# test: path/to/dataset/images/test   \n",
    "\n",
    "# nc: 3 \n",
    "# names: ['car', 'person', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DATA_YAML = \"path/to/dataset.yaml\"\n",
    "TEST_DIR = \"path/to/test\" \n",
    "OUTPUT_CSV = \"submission.csv\" \n",
    "\n",
    "model = YOLO(\"yolov11n.pt\")\n",
    "\n",
    "model.train(\n",
    "    data=DATA_YAML,  \n",
    "    epochs=10,     \n",
    "    imgsz=640,    \n",
    "    batch=16,      \n",
    "    lr0=1e-3,      \n",
    "    momentum=0.9,  \n",
    "    augment=True     \n",
    ")\n",
    "\n",
    "model_path = \"runs/train/exp/weights/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "test_images = [f for f in os.listdir(TEST_DIR) if f.endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for img_name in test_images:\n",
    "    img_path = os.path.join(TEST_DIR, img_name)\n",
    "    results = model(img_path)\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes.data:\n",
    "            x1, y1, x2, y2, conf, cls = box.tolist()\n",
    "            predictions.append((img_name, int(cls), conf, x1, y1, x2, y2))\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"class\", \"confidence\", \"x1\", \"y1\", \"x2\", \"y2\"])\n",
    "df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "TEST_CSV = \"test.csv\"\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "OUTPUT_DIR = \"vit_finetuned\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "VAL_SPLIT = 0.1\n",
    "NUM_WORKERS = 4\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "labels_unique = sorted(train_df.iloc[:, 1].unique())\n",
    "label2id = {lab: i for i, lab in enumerate(labels_unique)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "train_df['label_id'] = train_df.iloc[:, 1].map(label2id).astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_split, val_df = train_test_split(train_df, test_size=VAL_SPLIT, stratify=train_df['label_id'], random_state=SEED)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, img_col=0, label_col=None, transform=None):\n",
    "        self.paths = df.iloc[:, img_col].values.tolist()\n",
    "        self.labels = None if label_col is None else df[label_col].values.tolist()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.labels is None:\n",
    "            return {\"pixel_values\": img}\n",
    "        return {\"pixel_values\": img, \"labels\": int(self.labels[idx])}\n",
    "\n",
    "train_dataset = ImageDataset(train_df_split, img_col=0, label_col='label_id', transform=transform_train)\n",
    "val_dataset = ImageDataset(val_df, img_col=0, label_col='label_id', transform=transform_val)\n",
    "test_dataset = ImageDataset(test_df, img_col=0, label_col=None, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "num_labels = len(labels_unique)\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.06 * total_steps), num_training_steps=total_steps)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total if total > 0 else 0.0\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(OUTPUT_DIR)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy().tolist()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "pred_labels = [id2label[p] for p in all_preds]\n",
    "out_df = test_df.copy()\n",
    "out_df['label'] = pred_labels\n",
    "out_df.to_csv(\"test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "INPUT_CSV = \"images.csv\"\n",
    "PATH_COL = \"path\"\n",
    "OUT_CSV = \"image_embeddings.csv\"\n",
    "OUT_NPZ = \"image_embeddings.npz\"\n",
    "MODEL_NAME = \"facebook/dinov2-base\"\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "embs = []\n",
    "paths = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        batch_paths = df[PATH_COL].iloc[i : i + BATCH_SIZE].tolist()\n",
    "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        if getattr(outputs, \"pooler_output\", None) is not None:\n",
    "            batch_emb = outputs.pooler_output.cpu().numpy()\n",
    "        else:\n",
    "            last = outputs.last_hidden_state\n",
    "            if last is None:\n",
    "                raise ValueError(\"No embeddings in model output\")\n",
    "            if last.shape[1] >= 1:\n",
    "                batch_emb = last[:, 0, :].cpu().numpy()\n",
    "            else:\n",
    "                batch_emb = last.mean(dim=1).cpu().numpy()\n",
    "        embs.append(batch_emb)\n",
    "        paths.extend(batch_paths)\n",
    "\n",
    "emb_all = np.vstack(embs)\n",
    "emb_dim = emb_all.shape[1]\n",
    "cols = [\"path\"] + [f\"emb_{j}\" for j in range(emb_dim)]\n",
    "out_df = pd.DataFrame(np.concatenate([np.array(paths)[:, None], emb_all.astype(np.float32)], axis=1), columns=cols)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "np.savez_compressed(OUT_NPZ, paths=np.array(paths), embeddings=emb_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "INPUT_CSV = \"images.csv\"\n",
    "PATH_COL = \"path\"\n",
    "OUT_CSV = \"image_embeddings_single.csv\"\n",
    "OUT_NPZ = \"image_embeddings_single.npz\"\n",
    "MODEL_NAME = \"facebook/dinov2-base\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "paths = []\n",
    "embs = []\n",
    "with torch.no_grad():\n",
    "    for p in tqdm(df[PATH_COL].tolist(), desc=\"images\"):\n",
    "        image = Image.open(p).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "        if getattr(outputs, \"pooler_output\", None) is not None:\n",
    "            emb = outputs.pooler_output[0].cpu().numpy()\n",
    "        else:\n",
    "            last = outputs.last_hidden_state\n",
    "            if last.shape[1] >= 1:\n",
    "                emb = last[0, 0, :].cpu().numpy()\n",
    "            else:\n",
    "                emb = last.mean(dim=1)[0].cpu().numpy()\n",
    "        paths.append(p)\n",
    "        embs.append(emb.astype(np.float32))\n",
    "\n",
    "emb_all = np.vstack(embs) if len(embs) > 0 else np.zeros((0, 0), dtype=np.float32)\n",
    "emb_dim = emb_all.shape[1] if emb_all.ndim == 2 else 0\n",
    "cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "out_df = pd.DataFrame(emb_all, columns=cols)\n",
    "out_df.insert(0, PATH_COL, paths)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "np.savez_compressed(OUT_NPZ, paths=np.array(paths), embeddings=emb_all)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
