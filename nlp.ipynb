{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf + SVD + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_svd_encode(texts, max_features=20000, n_components=256, random_state=42):\n",
    "    vect = TfidfVectorizer(max_features=max_features, ngram_range=(1,2), min_df=3)\n",
    "    X = vect.fit_transform(texts)                      \n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
    "    emb = svd.fit_transform(X)                          \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_embedding_features(df,id_col=\"id\",text_col=\"text\",emb_col=\"embedding\",take_dim_in_csv=32, kmeans_clusters=32,pca_components=16,topk_cluster_dists=3,tfidf_topk=3,random_state=42):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    # emb_list = [ _parse_embedding(x) for x in df[emb_col].values ]\n",
    "    dims = [e.size for e in emb_list]\n",
    "    if len(set(dims)) != 1:\n",
    "        raise ValueError(f\"Embeddings have different dimensions: unique dims = {set(dims)}\")\n",
    "    emb_dim = dims[0]\n",
    "    emb_all = np.vstack(emb_list).astype(np.float32) \n",
    "    n = emb_all.shape[0]\n",
    "\n",
    "    texts = df[text_col].fillna(\"\").astype(str).values\n",
    "    text_len_words = np.array([ len(t.split()) for t in texts ], dtype=np.int32)\n",
    "    text_len_chars = np.array([ len(t) for t in texts ], dtype=np.int32)\n",
    "    has_text = (text_len_words > 0).astype(np.uint8)\n",
    "\n",
    "    emb_l2 = np.linalg.norm(emb_all, axis=1)                  \n",
    "    emb_mean_scalar = emb_all.mean(axis=1)                      \n",
    "    emb_std_scalar = emb_all.std(axis=1)\n",
    "    emb_max_scalar = emb_all.max(axis=1)\n",
    "    emb_min_scalar = emb_all.min(axis=1)\n",
    "    emb_median_scalar = np.median(emb_all, axis=1)\n",
    "    emb_q25 = np.percentile(emb_all, 25, axis=1)\n",
    "    emb_q75 = np.percentile(emb_all, 75, axis=1)\n",
    "\n",
    "    take_D = min(take_dim_in_csv, emb_dim)\n",
    "    emb_firstD = emb_all[:, :take_D]  \n",
    "\n",
    "    pca_feats = None\n",
    "    if pca_components is not None and pca_components > 0:\n",
    "        svd = TruncatedSVD(n_components=min(pca_components, emb_dim), random_state=random_state)\n",
    "        pca_feats = svd.fit_transform(emb_all)\n",
    "    else:\n",
    "        pca_feats = np.zeros((n, 0))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=kmeans_clusters, random_state=random_state, n_init=10)\n",
    "    kmeans.fit(emb_all)\n",
    "    clusters = kmeans.predict(emb_all)   \n",
    "    centers = kmeans.cluster_centers_ \n",
    "    dists = np.linalg.norm(emb_all[:, None, :] - centers[None, :, :], axis=2)\n",
    "    dist_to_centroid = dists[np.arange(n), clusters]\n",
    "    topk_dists = np.sort(dists, axis=1)[:, :topk_cluster_dists]\n",
    "\n",
    "    global_mean = emb_all.mean(axis=0)\n",
    "    emb_normed = normalize(emb_all)\n",
    "    global_normed = global_mean / (np.linalg.norm(global_mean) + 1e-12)\n",
    "    cos_to_global = (emb_normed @ global_normed).reshape(-1)\n",
    "\n",
    "    tfidf_top_tokens = [ [] for _ in range(n) ]\n",
    "    if tfidf_topk is not None and tfidf_topk > 0:\n",
    "        vect = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=1)\n",
    "        X_tfidf = vect.fit_transform(texts)     \n",
    "        feature_names = np.array(vect.get_feature_names_out())\n",
    "        for i in range(n):\n",
    "            row = X_tfidf.getrow(i)\n",
    "            if row.nnz == 0:\n",
    "                tfidf_top_tokens[i] = []\n",
    "            else:\n",
    "                data = row.data\n",
    "                cols = row.indices\n",
    "                order = np.argsort(-data)\n",
    "                top_idx = cols[order][:tfidf_topk]\n",
    "                tfidf_top_tokens[i] = feature_names[top_idx].tolist()\n",
    "\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        row = {\n",
    "            id_col: df.loc[i, id_col],\n",
    "            \"text_len_words\": int(text_len_words[i]),\n",
    "            \"text_len_chars\": int(text_len_chars[i]),\n",
    "            \"has_text\": int(has_text[i]),\n",
    "            \"emb_l2\": float(emb_l2[i]),\n",
    "            \"emb_mean_scalar\": float(emb_mean_scalar[i]),\n",
    "            \"emb_std_scalar\": float(emb_std_scalar[i]),\n",
    "            \"emb_max_scalar\": float(emb_max_scalar[i]),\n",
    "            \"emb_min_scalar\": float(emb_min_scalar[i]),\n",
    "            \"emb_median_scalar\": float(emb_median_scalar[i]),\n",
    "            \"emb_q25\": float(emb_q25[i]),\n",
    "            \"emb_q75\": float(emb_q75[i]),\n",
    "            \"cluster\": int(clusters[i]),\n",
    "            \"dist_to_centroid\": float(dist_to_centroid[i]),\n",
    "            \"cos_to_global_mean\": float(cos_to_global[i]),\n",
    "        }\n",
    "        for k in range(topk_cluster_dists):\n",
    "            row[f\"centroid_dist_{k}\"] = float(topk_dists[i, k])\n",
    "\n",
    "        for j in range(take_D):\n",
    "            row[f\"emb_comp_{j}\"] = float(emb_firstD[i, j])\n",
    "\n",
    "        for j in range(pca_feats.shape[1]):\n",
    "            row[f\"pca_{j}\"] = float(pca_feats[i, j])\n",
    "\n",
    "        row[\"tfidf_top_tokens\"] = \";\".join(tfidf_top_tokens[i]) if tfidf_top_tokens[i] else \"\"\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    features_df = pd.DataFrame(rows)\n",
    "\n",
    "    meta = {\n",
    "        \"n_rows\": n,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"take_dim_in_csv\": take_D,\n",
    "        \"kmeans_clusters\": kmeans_clusters,\n",
    "        \"pca_components\": pca_feats.shape[1],\n",
    "        \"tfidf_topk\": tfidf_topk\n",
    "    }\n",
    "\n",
    "    return features_df, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "reviews = pd.read_csv('reviews.tsv', sep='\\t') \n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"slovers/sentiment-roberta-large-ru\",\n",
    "    tokenizer=\"slovers/sentiment-roberta-large-ru\",\n",
    "    device=device,\n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "label_to_score = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "}\n",
    "\n",
    "def get_sentiment_label(text):\n",
    "    result = sentiment_pipeline(text[:512]) \n",
    "    label = result[0]['label'].lower()\n",
    "    return label_to_score.get(label, 1)\n",
    "\n",
    "tqdm.pandas(desc=\"Processing Sentiment\")\n",
    "reviews['sentiment'] = reviews['text'].progress_apply(get_sentiment_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert for task of text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "OUTPUT_DIR = \"fine_tuned_model\"\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, text_col=\"text\", label_col=\"label\", max_length=256):\n",
    "        self.texts = dataframe[text_col].fillna(\"\").astype(str).tolist()\n",
    "        self.labels = dataframe[label_col].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, max_length=self.max_length, padding=False)\n",
    "        enc[\"labels\"] = int(self.labels[idx])\n",
    "        return enc\n",
    "\n",
    "train_dataset = TextDataset(df, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset = TextDataset(df_test, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.06*total_steps), num_training_steps=total_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        probs = softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        preds = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_probs.extend(probs.tolist())\n",
    "\n",
    "labels_arr = np.array(all_labels)\n",
    "preds_arr = np.array(all_preds)\n",
    "probs_arr = np.array(all_probs)\n",
    "\n",
    "acc = accuracy_score(labels_arr, preds_arr)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels_arr, preds_arr, average=\"binary\", zero_division=0)\n",
    "auc = roc_auc_score(labels_arr, probs_arr)\n",
    "\n",
    "print(f\"accuracy: {acc:.6f}\")\n",
    "print(f\"precision: {precision:.6f}\")\n",
    "print(f\"recall: {recall:.6f}\")\n",
    "print(f\"f1: {f1:.6f}\")\n",
    "print(f\"roc_auc: {auc:.6f}\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df['text'].fillna('').astype(str).to_list()\n",
    "        self.label = df['label'].astype(int).to_list()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.text[idx])\n",
    "        enc['label'] = self.label[idx]\n",
    "        return enc\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Data(train, tokenizer)\n",
    "val_set = Data(val, tokenizer)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
    "train_loader = DataLoader(train_set, batch = 16, num_workers = -1, shuffle = True, collate_fn = data_collator)\n",
    "val_loader = DataLoader(val_set, batch = 16, num_workers = -1, shuffle = False, collate_fn = data_collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "for i in tqdm(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            out = model(**batch)\n",
    "            loss = out.loss\n",
    "            preds = out.logits\n",
    "            probs = softmax(preds, dim=1)[:, 1].cpu().numpy()\n",
    "            preds = np.argmax(preds.cpu().numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/debarshichanda/bert-multi-label-text-classification\n",
    "\n",
    "https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "data = pd.read_csv('/kaggle/input/vseros-nlp-qual/train.tsv', sep = '\\t')\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(data['shortDescription'].tolist(), 'embeddings'):\n",
    "        token = tokenizer(text, padding = False, max_length = 512, return_tensors=\"pt\")\n",
    "        token = {k: v.to(device) for k, v in token.items()}\n",
    "        outputs = model(**token, output_hidden_states=True)\n",
    "        last_layer = outputs.hidden_states[-1]\n",
    "        emb = last_layer.mean(dim=1).squeeze(0).cpu().numpy() \n",
    "        embeddings.append(emb) \n",
    "\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/learn/llm-course/chapter3/4\n",
    "\n",
    "https://huggingface.co/collections/ai-forever/sentenceembedders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация парафразом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "INPUT_CSV = \"input_texts.csv\"\n",
    "TEXT_COL = \"text\"\n",
    "MODEL_NAME = \"cointegrated/rut5-base-paraphraser\"\n",
    "OUT_CSV = \"paraphrases.csv\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_RETURN = 3\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_GEN_LENGTH = 128\n",
    "NUM_BEAMS = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "paraphrases_cols = [f\"paraphrase_{i+1}\" for i in range(NUM_RETURN)]\n",
    "rows = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"paraphrase\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        enc = tokenizer(batch, truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        gen = model.generate(**enc, max_length=MAX_GEN_LENGTH, num_beams=NUM_BEAMS, num_return_sequences=NUM_RETURN, early_stopping=True)\n",
    "        gen = gen.cpu().numpy()\n",
    "        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for j in range(len(batch)):\n",
    "            start = j * NUM_RETURN\n",
    "            paraphrases = decoded[start:start + NUM_RETURN]\n",
    "            row = {\"text\": batch[j]}\n",
    "            for k, p in enumerate(paraphrases):\n",
    "                row[paraphrases_cols[k]] = p\n",
    "            rows.append(row)\n",
    "\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммаризация с помощью Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "OUTPUT_DIR = \"sft_sum_model\"\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")  # столбцы: 'query', 'summary'\n",
    "df = df.dropna(subset=[\"summary\"])  # обязать target\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_batch(examples):\n",
    "    inputs = [str(x) for x in examples[\"query\"]]\n",
    "    targets = [str(x) for x in examples[\"summary\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    label_ids = labels[\"input_ids\"]\n",
    "    label_ids = [[(tok if tok != tokenizer.pad_token_id else -100) for tok in lbl] for lbl in label_ids]\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n",
    "\n",
    "ds_tok = ds.map(preprocess_batch, batched=True, remove_columns=ds.column_names)\n",
    "ds_tok = ds_tok.filter(lambda x: len([t for t in x[\"labels\"] if t != -100]) > 0)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_steps=50,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tok.shuffle(seed=42).select(range(int(0.9*len(ds_tok)))),\n",
    "    eval_dataset=ds_tok.shuffle(seed=42).select(range(int(0.9*len(ds_tok)), len(ds_tok))),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
